{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1748235263280,
     "user": {
      "displayName": "YEJI",
      "userId": "07260244821978580970"
     },
     "user_tz": -540
    },
    "id": "73uIDG-pWF8K",
    "outputId": "ae718af6-4c36-4806-865d-ffab747109af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14573,
     "status": "ok",
     "timestamp": 1748235279371,
     "user": {
      "displayName": "YEJI",
      "userId": "07260244821978580970"
     },
     "user_tz": -540
    },
    "id": "vPLepSHwAIrB",
    "outputId": "e8bd434a-8943-4afd-fec7-9884e90cbc45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting madgrad\n",
      "  Downloading madgrad-1.3.tar.gz (7.9 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting openpyxl==3.0.9\n",
      "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting xlrd==2.0.1\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting tqdm==4.67.1\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting regex==2023.12.25\n",
      "  Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting nltk==3.9.1\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting et-xmlfile (from openpyxl==3.0.9)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\이예지학부휴학통계학과\\downloads\\conference\\.venv\\lib\\site-packages (from tqdm==4.67.1) (0.4.6)\n",
      "Collecting click (from nltk==3.9.1)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk==3.9.1)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.2.6-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\이예지학부휴학통계학과\\downloads\\conference\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\이예지학부휴학통계학과\\downloads\\conference\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 19.7 MB/s eta 0:00:00\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 897.5/897.5 kB 42.4 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.7/11.6 MB 18.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.1/11.6 MB 17.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.7/11.6 MB 16.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.6-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 3.7/12.9 MB 18.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.9 MB 15.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/12.9 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 13.3 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Building wheels for collected packages: madgrad\n",
      "  Building wheel for madgrad (pyproject.toml): started\n",
      "  Building wheel for madgrad (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for madgrad: filename=madgrad-1.3-py3-none-any.whl size=11970 sha256=771aae7245a629bbf6d42c4d6dd69679291158eb3c927c72d3e900ad516a7fe6\n",
      "  Stored in directory: c:\\users\\이예지학부휴학통계학과\\appdata\\local\\pip\\cache\\wheels\\9d\\d8\\f7\\f29e91274f40f339d572cc917a8a6e5307fc0167c682f45655\n",
      "Successfully built madgrad\n",
      "Installing collected packages: pytz, xlrd, tzdata, tqdm, regex, numpy, madgrad, joblib, et-xmlfile, click, sacremoses, pandas, openpyxl, nltk\n",
      "\n",
      "   ----------------------------------------  0/14 [pytz]\n",
      "   ----------------------------------------  0/14 [pytz]\n",
      "   -- -------------------------------------  1/14 [xlrd]\n",
      "   ----- ----------------------------------  2/14 [tzdata]\n",
      "   ----- ----------------------------------  2/14 [tzdata]\n",
      "   -------- -------------------------------  3/14 [tqdm]\n",
      "   -------- -------------------------------  3/14 [tqdm]\n",
      "   ----------- ----------------------------  4/14 [regex]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------- -------------------------  5/14 [numpy]\n",
      "   -------------------- -------------------  7/14 [joblib]\n",
      "   -------------------- -------------------  7/14 [joblib]\n",
      "   -------------------- -------------------  7/14 [joblib]\n",
      "   -------------------- -------------------  7/14 [joblib]\n",
      "   -------------------- -------------------  7/14 [joblib]\n",
      "   -------------------- -------------------  7/14 [joblib]\n",
      "   ------------------------- --------------  9/14 [click]\n",
      "   ------------------------- --------------  9/14 [click]\n",
      "   ---------------------------- ----------- 10/14 [sacremoses]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ------------------------------- -------- 11/14 [pandas]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ---------------------------------- ----- 12/14 [openpyxl]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ------------------------------------- -- 13/14 [nltk]\n",
      "   ---------------------------------------- 14/14 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 et-xmlfile-2.0.0 joblib-1.5.1 madgrad-1.3 nltk-3.9.1 numpy-2.2.6 openpyxl-3.0.9 pandas-2.2.3 pytz-2025.2 regex-2023.12.25 sacremoses-0.1.1 tqdm-4.67.1 tzdata-2025.2 xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# 필수 라이브러리 설치 (Colab 기본 환경 호환 유지)\n",
    "!pip install sacremoses madgrad pandas openpyxl==3.0.9 xlrd==2.0.1 tqdm==4.67.1 regex==2023.12.25 nltk==3.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PesuFgqIXYlb",
    "outputId": "d96107ee-d60b-497d-c241-1fdcd29a6c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-26 04:55:05.513794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748235305.535669     702 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748235305.541355     702 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-26 04:55:05.561524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Splitting data according to data/data_split.json\n",
      "Starting Train Data\n",
      "Deleting .json files in data/train/...\n",
      "Finished\n",
      "Copying files...\n"
     ]
    }
   ],
   "source": [
    "# 파일이 제대로 작동하는지 확인\n",
    "\n",
    "import sys\n",
    "\n",
    "!{sys.executable} preprocess.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능 여부: True\n",
      "사용 중인 GPU: NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"사용 중인 GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"사용 불가\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2209443,
     "status": "ok",
     "timestamp": 1748181227398,
     "user": {
      "displayName": "YEJI",
      "userId": "07260244821978580970"
     },
     "user_tz": -540
    },
    "id": "dSNbzX6XFfTf",
    "outputId": "cd830a2e-e6ce-4107-dd92-84e9b94f9d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 1/95, loss: 0.3399309694766998\n",
      "epoch: 0, batch: 2/95, loss: 0.23647718578577043\n",
      "epoch: 0, batch: 3/95, loss: 0.1881980761885643\n",
      "epoch: 0, batch: 4/95, loss: 0.18337729722261428\n",
      "epoch: 0, batch: 5/95, loss: 0.17502919882535933\n",
      "epoch: 0, batch: 6/95, loss: 0.16858744472265244\n",
      "epoch: 0, batch: 7/95, loss: 0.1764312729239464\n",
      "epoch: 0, batch: 8/95, loss: 0.14490818232297897\n",
      "epoch: 0, batch: 9/95, loss: 0.1480210840702057\n",
      "epoch: 0, batch: 10/95, loss: 0.1565759763121605\n",
      "epoch: 0, batch: 11/95, loss: 0.1631219767034054\n",
      "epoch: 0, batch: 12/95, loss: 0.15240549817681312\n",
      "epoch: 0, batch: 13/95, loss: 0.1521803118288517\n",
      "epoch: 0, batch: 14/95, loss: 0.13422973155975343\n",
      "epoch: 0, batch: 15/95, loss: 0.14452400133013726\n",
      "epoch: 0, batch: 16/95, loss: 0.14694433361291886\n",
      "epoch: 0, batch: 17/95, loss: 0.15022060945630072\n",
      "epoch: 0, batch: 18/95, loss: 0.14910632669925689\n",
      "epoch: 0, batch: 19/95, loss: 0.1391106925904751\n",
      "epoch: 0, batch: 20/95, loss: 0.14698492586612702\n",
      "epoch: 0, batch: 21/95, loss: 0.1490918792784214\n",
      "epoch: 0, batch: 22/95, loss: 0.14779582470655442\n",
      "epoch: 0, batch: 23/95, loss: 0.1302958086133003\n",
      "epoch: 0, batch: 24/95, loss: 0.14417894408106804\n",
      "epoch: 0, batch: 25/95, loss: 0.13324507549405099\n",
      "epoch: 0, batch: 26/95, loss: 0.12955972477793692\n",
      "epoch: 0, batch: 27/95, loss: 0.12975441366434098\n",
      "epoch: 0, batch: 28/95, loss: 0.1407700166106224\n",
      "epoch: 0, batch: 29/95, loss: 0.13836656808853148\n",
      "epoch: 0, batch: 30/95, loss: 0.14496645480394363\n",
      "epoch: 0, batch: 31/95, loss: 0.14908725321292876\n",
      "epoch: 0, batch: 32/95, loss: 0.12924303710460663\n",
      "epoch: 0, batch: 33/95, loss: 0.13285928964614868\n",
      "epoch: 0, batch: 34/95, loss: 0.139988324791193\n",
      "epoch: 0, batch: 35/95, loss: 0.1255621552467346\n",
      "epoch: 0, batch: 36/95, loss: 0.12998268380761147\n",
      "epoch: 0, batch: 37/95, loss: 0.14080625846982003\n",
      "epoch: 0, batch: 38/95, loss: 0.13552936166524887\n",
      "epoch: 0, batch: 39/95, loss: 0.14975724667310714\n",
      "epoch: 0, batch: 40/95, loss: 0.13723631277680398\n",
      "epoch: 0, batch: 41/95, loss: 0.1272553473711014\n",
      "epoch: 0, batch: 42/95, loss: 0.1334388203918934\n",
      "epoch: 0, batch: 43/95, loss: 0.1311638429760933\n",
      "epoch: 0, batch: 44/95, loss: 0.1436328612267971\n",
      "epoch: 0, batch: 45/95, loss: 0.1327366702258587\n",
      "epoch: 0, batch: 46/95, loss: 0.13270925134420394\n",
      "epoch: 0, batch: 47/95, loss: 0.13398547992110252\n",
      "epoch: 0, batch: 48/95, loss: 0.13219813257455826\n",
      "epoch: 0, batch: 49/95, loss: 0.13846639320254325\n",
      "epoch: 0, batch: 50/95, loss: 0.13131171613931655\n",
      "epoch: 0, batch: 51/95, loss: 0.14238228127360344\n",
      "epoch: 0, batch: 52/95, loss: 0.12834276705980302\n",
      "epoch: 0, batch: 53/95, loss: 0.13442694619297982\n",
      "epoch: 0, batch: 54/95, loss: 0.137675691395998\n",
      "epoch: 0, batch: 55/95, loss: 0.13053296953439714\n",
      "epoch: 0, batch: 56/95, loss: 0.135941468924284\n",
      "epoch: 0, batch: 57/95, loss: 0.13334739431738854\n",
      "epoch: 0, batch: 58/95, loss: 0.12495449632406234\n",
      "epoch: 0, batch: 59/95, loss: 0.12474664598703385\n",
      "epoch: 0, batch: 60/95, loss: 0.14749409034848213\n",
      "epoch: 0, batch: 61/95, loss: 0.13349539563059806\n",
      "epoch: 0, batch: 62/95, loss: 0.1329281248152256\n",
      "epoch: 0, batch: 63/95, loss: 0.12783021479845047\n",
      "epoch: 0, batch: 64/95, loss: 0.11722165048122406\n",
      "epoch: 0, batch: 65/95, loss: 0.13559112474322318\n",
      "epoch: 0, batch: 66/95, loss: 0.13599014580249785\n",
      "epoch: 0, batch: 67/95, loss: 0.12573972940444947\n",
      "epoch: 0, batch: 68/95, loss: 0.13233756572008132\n",
      "epoch: 0, batch: 69/95, loss: 0.1354110911488533\n",
      "epoch: 0, batch: 70/95, loss: 0.12624984979629517\n",
      "epoch: 0, batch: 71/95, loss: 0.12667239606380462\n",
      "epoch: 0, batch: 72/95, loss: 0.13138198778033255\n",
      "epoch: 0, batch: 73/95, loss: 0.137634839117527\n",
      "epoch: 0, batch: 74/95, loss: 0.1362385109066963\n",
      "epoch: 0, batch: 75/95, loss: 0.12074097245931625\n",
      "epoch: 0, batch: 76/95, loss: 0.13044911175966262\n",
      "epoch: 0, batch: 77/95, loss: 0.13943733721971513\n",
      "epoch: 0, batch: 78/95, loss: 0.1212122082710266\n",
      "epoch: 0, batch: 79/95, loss: 0.12866669669747352\n",
      "epoch: 0, batch: 80/95, loss: 0.1302643470466137\n",
      "epoch: 0, batch: 81/95, loss: 0.12040887922048568\n",
      "epoch: 0, batch: 82/95, loss: 0.12976273372769356\n",
      "epoch: 0, batch: 83/95, loss: 0.1183674693107605\n",
      "epoch: 0, batch: 84/95, loss: 0.12044882029294968\n",
      "epoch: 0, batch: 85/95, loss: 0.13796710968017578\n",
      "epoch: 0, batch: 86/95, loss: 0.12767929434776307\n",
      "epoch: 0, batch: 87/95, loss: 0.12212395146489144\n",
      "epoch: 0, batch: 88/95, loss: 0.13335857689380645\n",
      "epoch: 0, batch: 89/95, loss: 0.1317548520863056\n",
      "epoch: 0, batch: 90/95, loss: 0.12872868031263351\n",
      "epoch: 0, batch: 91/95, loss: 0.11335858926177025\n",
      "epoch: 0, batch: 92/95, loss: 0.12245927900075912\n",
      "epoch: 0, batch: 93/95, loss: 0.12403652220964431\n",
      "epoch: 0, batch: 94/95, loss: 0.12149255126714706\n",
      "epoch: 0, batch: 94/95, loss: 0.12574237659573556\n",
      "Epoch train time: 1195.2505722045898 seconds\n",
      "\n",
      "Validating-> mb: 0, mode score: 0.05801857709884643, gloss score: 0.3388510704040527, timing score: 0.04228346943855286\n",
      "Validating-> mb: 1, mode score: 0.05767350196838379, gloss score: 0.33015025854110713, timing score: 0.04239512532949448\n",
      "Validating-> mb: 2, mode score: 0.05514167149861654, gloss score: 0.34724808533986407, timing score: 0.04380912582079569\n",
      "Validating-> mb: 3, mode score: 0.05295740514993668, gloss score: 0.33620327711105347, timing score: 0.0450116217136383\n",
      "Validating-> mb: 4, mode score: 0.05474966287612916, gloss score: 0.33693359375, timing score: 0.046938852071762086\n",
      "Validating-> mb: 5, mode score: 0.05679684082667034, gloss score: 0.33598742882410687, timing score: 0.04536178906758626\n",
      "Validating-> mb: 6, mode score: 0.05597944344793048, gloss score: 0.33625330584389823, timing score: 0.045253727265766694\n",
      "Validating-> mb: 7, mode score: 0.05579896792769433, gloss score: 0.34145257771015164, timing score: 0.04398643933236599\n",
      "Validating-> mb: 8, mode score: 0.056071529785792044, gloss score: 0.3478583176930745, timing score: 0.043052752481566536\n",
      "Validating-> mb: 9, mode score: 0.055968602299690265, gloss score: 0.34381480216979976, timing score: 0.04255172610282898\n",
      "Validating-> mb: 10, mode score: 0.05494280836798929, gloss score: 0.34111437147313894, timing score: 0.04103799516504461\n",
      "Validating-> mb: 11, mode score: 0.05504544923702876, gloss score: 0.3406340301036834, timing score: 0.042200349271297455\n",
      "Validating-> mb: 12, mode score: 0.05472982571675227, gloss score: 0.33876552948584915, timing score: 0.04310608414503244\n",
      "Validating-> mb: 13, mode score: 0.053816362151077816, gloss score: 0.333970774923052, timing score: 0.04306621998548508\n",
      "Validating-> mb: 14, mode score: 0.053536968827247615, gloss score: 0.3340990352630614, timing score: 0.043294070959091185\n",
      "Validating-> mb: 15, mode score: 0.053582150302827355, gloss score: 0.3327540144324302, timing score: 0.04355275053530931\n",
      "Validating-> mb: 16, mode score: 0.0535940747050678, gloss score: 0.33139442275552183, timing score: 0.04410110729582169\n",
      "Validating-> mb: 17, mode score: 0.05340532941950692, gloss score: 0.33019243213865485, timing score: 0.0440540881620513\n",
      "Validating-> mb: 18, mode score: 0.0537665666718232, gloss score: 0.33079628442463116, timing score: 0.04384623718889136\n",
      "Validating-> mb: 19, mode score: 0.053239779621362684, gloss score: 0.32882873415946956, timing score: 0.04350790798664093\n",
      "Validating-> mb: 20, mode score: 0.053087013534137174, gloss score: 0.32857588699885776, timing score: 0.04337234014556521\n",
      "Validating-> mb: 21, mode score: 0.05313344665549018, gloss score: 0.330255604874004, timing score: 0.04394081641327251\n",
      "Validating-> mb: 22, mode score: 0.0530464152926984, gloss score: 0.32709882984990657, timing score: 0.04397144952545995\n",
      "Validating-> mb: 23, mode score: 0.05241020297010739, gloss score: 0.32579566836357116, timing score: 0.04344861743350823\n",
      "Validating-> mb: 24, mode score: 0.05230815839767455, gloss score: 0.32637343120574946, timing score: 0.04325210225582122\n",
      "Validating-> mb: 25, mode score: 0.05214183846345314, gloss score: 0.32566442122826206, timing score: 0.04266870755415696\n",
      "Validating-> mb: 26, mode score: 0.05210826297601063, gloss score: 0.32623088977955, timing score: 0.04246109492248958\n",
      "Validating-> mb: 27, mode score: 0.05226247449006353, gloss score: 0.3271014596734728, timing score: 0.042247701223407465\n",
      "Validating-> mb: 28, mode score: 0.05236080730783528, gloss score: 0.32838644159251246, timing score: 0.042587935821763394\n",
      "Validating-> mb: 29, mode score: 0.052379749317963915, gloss score: 0.3284068274497986, timing score: 0.04266924649477004\n",
      "Validating-> mb: 30, mode score: 0.05237584700507502, gloss score: 0.33023340932784545, timing score: 0.04296969165725092\n",
      "Validating-> mb: 31, mode score: 0.052300538495182985, gloss score: 0.32871736362576487, timing score: 0.044083089102059596\n",
      "Validating-> mb: 32, mode score: 0.05196047380115046, gloss score: 0.3277818087375525, timing score: 0.04504608647389845\n",
      "Validating-> mb: 33, mode score: 0.05189926650594262, gloss score: 0.32740597865160775, timing score: 0.04534467975882922\n",
      "Validating-> mb: 34, mode score: 0.05176534559045518, gloss score: 0.3278796298163278, timing score: 0.04543061247893741\n",
      "Validating-> mb: 35, mode score: 0.051935802151759455, gloss score: 0.3271718151039547, timing score: 0.04554809389842881\n",
      "Validating-> mb: 36, mode score: 0.052134105563163745, gloss score: 0.3278547196774869, timing score: 0.04546428065042238\n",
      "Validating-> mb: 37, mode score: 0.05209122558957651, gloss score: 0.32620490789413453, timing score: 0.0450745287694429\n",
      "Validating-> mb: 38, mode score: 0.05210148241275395, gloss score: 0.3262181777220506, timing score: 0.045171586596048786\n",
      "Validating-> mb: 39, mode score: 0.05235973007977007, gloss score: 0.32456693172454837, timing score: 0.04552255518734454\n",
      "Validating-> mb: 40, mode score: 0.05216653521467998, gloss score: 0.3255905977109584, timing score: 0.045169426246387194\n",
      "Validating-> mb: 41, mode score: 0.05207081877049944, gloss score: 0.3255798992656526, timing score: 0.04481158469404492\n",
      "Validating-> mb: 42, mode score: 0.05180038477099217, gloss score: 0.3257933478022731, timing score: 0.0445624074963636\n",
      "Validating-> mb: 43, mode score: 0.05188660730015146, gloss score: 0.32495201771909543, timing score: 0.04453010565855285\n",
      "Validating-> mb: 44, mode score: 0.05188154008653427, gloss score: 0.3255635860231188, timing score: 0.044246477948294734\n",
      "Validating-> mb: 45, mode score: 0.05206344555253567, gloss score: 0.32504466461098713, timing score: 0.04443472584952477\n",
      "Validating-> mb: 46, mode score: 0.051954208092486596, gloss score: 0.3261475836977046, timing score: 0.0441923104702158\n",
      "Validating-> mb: 47, mode score: 0.05176271616170802, gloss score: 0.3261776189009349, timing score: 0.04385187483082214\n",
      "Validating-> mb: 48, mode score: 0.051906391856621714, gloss score: 0.3259161798321471, timing score: 0.043982158692515615\n",
      "Validating-> mb: 49, mode score: 0.05200805681943892, gloss score: 0.3258059768676758, timing score: 0.043917754709720605\n",
      "Validating-> mb: 50, mode score: 0.05198913000377953, gloss score: 0.3250760831084906, timing score: 0.04433055934952754\n",
      "Validating-> mb: 51, mode score: 0.052071437870080645, gloss score: 0.32650847572546743, timing score: 0.04443038673355028\n",
      "Validating-> mb: 52, mode score: 0.05198426224150746, gloss score: 0.3255138878552419, timing score: 0.04440978703633793\n",
      "Validating-> mb: 53, mode score: 0.05211488990871994, gloss score: 0.32527999348110626, timing score: 0.044527669471723055\n",
      "Validating-> mb: 54, mode score: 0.0520954685861414, gloss score: 0.32574210036884654, timing score: 0.044538668556646864\n",
      "Validating-> mb: 55, mode score: 0.05232317681823456, gloss score: 0.32626730118479047, timing score: 0.04486154536051409\n",
      "Validating-> mb: 56, mode score: 0.052260258480122196, gloss score: 0.3259546192068803, timing score: 0.04495038243762233\n",
      "Validating-> mb: 57, mode score: 0.052227371674159466, gloss score: 0.32627599157136067, timing score: 0.04485686041157821\n",
      "Validating-> mb: 58, mode score: 0.05222968767255038, gloss score: 0.32482115171723447, timing score: 0.04516110107050103\n",
      "Validating-> mb: 59, mode score: 0.05227775012453396, gloss score: 0.3254008440176646, timing score: 0.04500656689206759\n",
      "Validating-> mb: 60, mode score: 0.0522745161271486, gloss score: 0.3241552118395196, timing score: 0.04546309489695752\n",
      "Validating-> mb: 61, mode score: 0.052232256772056694, gloss score: 0.32487237068914604, timing score: 0.04535965900267324\n",
      "Validating-> mb: 62, mode score: 0.052264678620156775, gloss score: 0.32466490760682126, timing score: 0.045297393770444956\n",
      "Validating-> mb: 63, mode score: 0.0523005946073681, gloss score: 0.32400273457169537, timing score: 0.04524981449358165\n",
      "Validating-> mb: 64, mode score: 0.052202931000636166, gloss score: 0.3241760477652917, timing score: 0.04514379538022554\n",
      "Validating-> mb: 65, mode score: 0.052161431493181164, gloss score: 0.3241738445831068, timing score: 0.04519225629893215\n",
      "Validating-> mb: 66, mode score: 0.05220460518082575, gloss score: 0.32455191185225307, timing score: 0.04518252953664579\n",
      "Validating-> mb: 67, mode score: 0.05217038458761046, gloss score: 0.3250979100956637, timing score: 0.04514938435133765\n",
      "Validating-> mb: 68, mode score: 0.05217538324819094, gloss score: 0.3252251258794812, timing score: 0.045576580624649476\n",
      "Validating-> mb: 69, mode score: 0.052073761778218396, gloss score: 0.32567759581974576, timing score: 0.045433644482067645\n",
      "Validating-> mb: 70, mode score: 0.051882139939657394, gloss score: 0.32532050508848376, timing score: 0.04534486125052813\n",
      "Validating-> mb: 71, mode score: 0.05176949890123472, gloss score: 0.3256900088654624, timing score: 0.0452659998089075\n",
      "Validating-> mb: 72, mode score: 0.051583514066591646, gloss score: 0.32525401768619067, timing score: 0.04512103097896052\n",
      "Validating-> mb: 73, mode score: 0.0516635845641832, gloss score: 0.32526075131184345, timing score: 0.0449906387441867\n",
      "Validating-> mb: 74, mode score: 0.05172515010833739, gloss score: 0.32557284355163574, timing score: 0.044902753988901756\n",
      "Validating-> mb: 75, mode score: 0.05181638347475151, gloss score: 0.32549411246651094, timing score: 0.04495905912236162\n",
      "Validating-> mb: 76, mode score: 0.05185482354907245, gloss score: 0.32819624256778074, timing score: 0.04478934937483303\n",
      "Validating-> mb: 77, mode score: 0.051835894202574694, gloss score: 0.3279872059822082, timing score: 0.04476047807779066\n",
      "Validating-> mb: 78, mode score: 0.05183567450016358, gloss score: 0.3285953615285173, timing score: 0.04477393102796771\n",
      "Validating-> mb: 79, mode score: 0.051767742857336985, gloss score: 0.3285716289281845, timing score: 0.04475698389112948\n",
      "Validating-> mb: 80, mode score: 0.05188751058813965, gloss score: 0.32871687235655606, timing score: 0.04501485831943558\n",
      "Validating-> mb: 81, mode score: 0.051863785124406564, gloss score: 0.3283891061457192, timing score: 0.044967624872196\n",
      "Validating-> mb: 82, mode score: 0.051800192300095596, gloss score: 0.3279305314443198, timing score: 0.0450370158416679\n",
      "Validating-> mb: 83, mode score: 0.05185395901401836, gloss score: 0.32791611495472134, timing score: 0.04494232755331765\n",
      "Validating-> mb: 84, mode score: 0.05189334136598248, gloss score: 0.3277755698035745, timing score: 0.04480992850135354\n",
      "Validating-> mb: 85, mode score: 0.05190110889285107, gloss score: 0.3284188303836556, timing score: 0.04486364237097806\n",
      "Validating-> mb: 86, mode score: 0.05189437506527734, gloss score: 0.3287204172419405, timing score: 0.04474178220348796\n",
      "Validating-> mb: 87, mode score: 0.05190638930282807, gloss score: 0.3287062888795679, timing score: 0.044743515415625136\n",
      "Validating-> mb: 88, mode score: 0.051905926191404946, gloss score: 0.32826959336741585, timing score: 0.044787757517246715\n",
      "Validating-> mb: 89, mode score: 0.05197306457493038, gloss score: 0.3276500230365329, timing score: 0.04479749702745014\n",
      "Validating-> mb: 90, mode score: 0.05191630200370326, gloss score: 0.3276819871022151, timing score: 0.0447218249132345\n",
      "Validating-> mb: 91, mode score: 0.05194183669012525, gloss score: 0.327640872416289, timing score: 0.044666222612495005\n",
      "Validating-> mb: 92, mode score: 0.05185496525097917, gloss score: 0.32764431866266397, timing score: 0.044646814689841316\n",
      "Validating-> mb: 93, mode score: 0.05189665536931219, gloss score: 0.327297720249663, timing score: 0.044550008406030364\n",
      "Validating-> mb: 94, mode score: 0.05180288098360362, gloss score: 0.3269651350222136, timing score: 0.04447735212351146\n",
      "Validating-> mb: 95, mode score: 0.05187568754578629, gloss score: 0.3274856227139632, timing score: 0.044316417630761856\n",
      "Validating-> mb: 96, mode score: 0.05196510089426924, gloss score: 0.32762015971940817, timing score: 0.04429675626386072\n",
      "Validating-> mb: 97, mode score: 0.051884431954549275, gloss score: 0.3280560578618731, timing score: 0.04430427548228477\n",
      "Validating-> mb: 98, mode score: 0.05194085791255488, gloss score: 0.32804261217213637, timing score: 0.044233458391343704\n",
      "Validating-> mb: 99, mode score: 0.05193326422572135, gloss score: 0.32748716783523557, timing score: 0.044242600351572035\n",
      "Validating-> mb: 100, mode score: 0.05187839039481512, gloss score: 0.32707745651207343, timing score: 0.044309067873671504\n",
      "Validating-> mb: 101, mode score: 0.051928225627132486, gloss score: 0.32766752874149996, timing score: 0.044249911664747725\n",
      "Validating-> mb: 102, mode score: 0.051883373243137465, gloss score: 0.328396815929598, timing score: 0.044248946170205056\n",
      "Validating-> mb: 103, mode score: 0.051826252043247215, gloss score: 0.32822519494937014, timing score: 0.0442409889629254\n",
      "Validating-> mb: 104, mode score: 0.05184006429853893, gloss score: 0.32754003570193335, timing score: 0.0441768232981364\n",
      "Validating-> mb: 105, mode score: 0.05181736344436429, gloss score: 0.32779877163329213, timing score: 0.044223829766489434\n",
      "Validating-> mb: 106, mode score: 0.05176295752280226, gloss score: 0.3282147472149858, timing score: 0.04428112813245471\n",
      "Validating-> mb: 107, mode score: 0.05183949158699424, gloss score: 0.32812964187728033, timing score: 0.044270391155172284\n",
      "Validating-> mb: 108, mode score: 0.05191563720550012, gloss score: 0.32861934626868006, timing score: 0.044501888697300485\n",
      "Validating-> mb: 109, mode score: 0.05204600949179042, gloss score: 0.3287840513749556, timing score: 0.04452862690795552\n",
      "Validating-> mb: 110, mode score: 0.052084178672180514, gloss score: 0.32841720194429963, timing score: 0.044501609496168194\n",
      "Validating-> mb: 111, mode score: 0.05204837532447916, gloss score: 0.3281707708324704, timing score: 0.04447845316358976\n",
      "Validating-> mb: 112, mode score: 0.051953464166253, gloss score: 0.32758108620095033, timing score: 0.04437633024907746\n",
      "Validating-> mb: 113, mode score: 0.052015162350838626, gloss score: 0.3272683298378659, timing score: 0.04440698328248242\n",
      "Validating-> mb: 114, mode score: 0.05199750721454619, gloss score: 0.32664066874462616, timing score: 0.04439234093479489\n",
      "Validating-> mb: 115, mode score: 0.051930185946924924, gloss score: 0.32684593488430147, timing score: 0.04430352130840566\n",
      "Val loss: 0.11052921231108151, (mean of mode: 0.05193018594692493, gloss: 0.3268459348843015, timing: 0.044303521308405654, pointing: 0.017401624149551577)\n",
      "\n",
      "\n",
      "Saving new best model at epoch 0 with val loss 0.1105\n",
      "epoch: 1, batch: 1/95, loss: 0.11727805137634277\n",
      "epoch: 1, batch: 2/95, loss: 0.11119686290621758\n",
      "epoch: 1, batch: 3/95, loss: 0.12196745872497558\n",
      "epoch: 1, batch: 4/95, loss: 0.1254350647330284\n",
      "epoch: 1, batch: 5/95, loss: 0.12201512828469277\n",
      "epoch: 1, batch: 6/95, loss: 0.12606082260608673\n",
      "epoch: 1, batch: 7/95, loss: 0.10234782323241234\n",
      "epoch: 1, batch: 8/95, loss: 0.12538862004876136\n",
      "epoch: 1, batch: 9/95, loss: 0.11786649376153946\n",
      "epoch: 1, batch: 10/95, loss: 0.11993604898452759\n",
      "epoch: 1, batch: 11/95, loss: 0.1267789199948311\n",
      "epoch: 1, batch: 12/95, loss: 0.11305283606052399\n",
      "epoch: 1, batch: 13/95, loss: 0.11856701225042343\n",
      "epoch: 1, batch: 14/95, loss: 0.1216222383081913\n",
      "epoch: 1, batch: 15/95, loss: 0.1261816456913948\n",
      "epoch: 1, batch: 16/95, loss: 0.11363274231553078\n",
      "epoch: 1, batch: 17/95, loss: 0.11708176508545876\n",
      "epoch: 1, batch: 18/95, loss: 0.11681722626090049\n",
      "epoch: 1, batch: 19/95, loss: 0.11565214172005653\n",
      "epoch: 1, batch: 20/95, loss: 0.11135359406471253\n",
      "epoch: 1, batch: 21/95, loss: 0.1193976454436779\n",
      "epoch: 1, batch: 22/95, loss: 0.11014494821429252\n",
      "epoch: 1, batch: 23/95, loss: 0.1129861481487751\n",
      "epoch: 1, batch: 24/95, loss: 0.11762555241584778\n",
      "epoch: 1, batch: 25/95, loss: 0.12609081342816353\n",
      "epoch: 1, batch: 26/95, loss: 0.11110118180513381\n",
      "epoch: 1, batch: 27/95, loss: 0.11737905517220497\n",
      "epoch: 1, batch: 28/95, loss: 0.1217933364212513\n",
      "epoch: 1, batch: 29/95, loss: 0.12117582708597183\n",
      "epoch: 1, batch: 30/95, loss: 0.12450875118374824\n",
      "epoch: 1, batch: 31/95, loss: 0.12035614177584648\n",
      "epoch: 1, batch: 32/95, loss: 0.11948667317628861\n",
      "epoch: 1, batch: 33/95, loss: 0.12176314666867256\n",
      "epoch: 1, batch: 34/95, loss: 0.119259774684906\n",
      "epoch: 1, batch: 35/95, loss: 0.11985007375478744\n",
      "epoch: 1, batch: 36/95, loss: 0.11319184973835945\n",
      "epoch: 1, batch: 37/95, loss: 0.12395763024687767\n",
      "epoch: 1, batch: 38/95, loss: 0.10631453022360801\n",
      "epoch: 1, batch: 39/95, loss: 0.11534464582800866\n",
      "epoch: 1, batch: 40/95, loss: 0.10671087726950645\n",
      "epoch: 1, batch: 41/95, loss: 0.11146200895309448\n",
      "epoch: 1, batch: 42/95, loss: 0.1158903107047081\n",
      "epoch: 1, batch: 43/95, loss: 0.11519650742411613\n",
      "epoch: 1, batch: 44/95, loss: 0.11496078595519066\n",
      "epoch: 1, batch: 45/95, loss: 0.12250736504793167\n",
      "epoch: 1, batch: 46/95, loss: 0.11827845349907876\n",
      "epoch: 1, batch: 47/95, loss: 0.1253514766693115\n",
      "epoch: 1, batch: 48/95, loss: 0.11154756993055344\n",
      "epoch: 1, batch: 49/95, loss: 0.11585047170519829\n",
      "epoch: 1, batch: 50/95, loss: 0.1238587811589241\n",
      "epoch: 1, batch: 51/95, loss: 0.10979404002428055\n",
      "epoch: 1, batch: 52/95, loss: 0.1215334564447403\n",
      "epoch: 1, batch: 53/95, loss: 0.10948409885168076\n",
      "epoch: 1, batch: 54/95, loss: 0.1101426549255848\n",
      "epoch: 1, batch: 55/95, loss: 0.12126426547765731\n",
      "epoch: 1, batch: 56/95, loss: 0.10767138227820397\n",
      "epoch: 1, batch: 57/95, loss: 0.1031818337738514\n",
      "epoch: 1, batch: 58/95, loss: 0.11581425741314888\n",
      "epoch: 1, batch: 59/95, loss: 0.1104087270796299\n",
      "epoch: 1, batch: 60/95, loss: 0.11426896303892135\n",
      "epoch: 1, batch: 61/95, loss: 0.10654831305146217\n",
      "epoch: 1, batch: 62/95, loss: 0.12496248632669449\n",
      "epoch: 1, batch: 63/95, loss: 0.11907864212989808\n",
      "epoch: 1, batch: 64/95, loss: 0.10275510177016259\n",
      "epoch: 1, batch: 65/95, loss: 0.10847449973225594\n",
      "epoch: 1, batch: 66/95, loss: 0.10841098949313163\n",
      "epoch: 1, batch: 67/95, loss: 0.11561197340488434\n",
      "epoch: 1, batch: 68/95, loss: 0.10385839119553567\n",
      "epoch: 1, batch: 69/95, loss: 0.10497426614165306\n",
      "epoch: 1, batch: 70/95, loss: 0.10309119075536728\n",
      "epoch: 1, batch: 71/95, loss: 0.1072993353009224\n",
      "epoch: 1, batch: 72/95, loss: 0.11554136201739311\n",
      "epoch: 1, batch: 73/95, loss: 0.11505444347858429\n",
      "epoch: 1, batch: 74/95, loss: 0.11146633997559548\n",
      "epoch: 1, batch: 75/95, loss: 0.10585047602653504\n",
      "epoch: 1, batch: 76/95, loss: 0.10262546464800834\n",
      "epoch: 1, batch: 77/95, loss: 0.10150695741176605\n",
      "epoch: 1, batch: 78/95, loss: 0.11041996106505395\n",
      "epoch: 1, batch: 79/95, loss: 0.09512484893202781\n",
      "epoch: 1, batch: 80/95, loss: 0.11562268808484077\n",
      "epoch: 1, batch: 81/95, loss: 0.11466753929853439\n",
      "epoch: 1, batch: 82/95, loss: 0.10993090569972992\n",
      "epoch: 1, batch: 83/95, loss: 0.11033599898219108\n",
      "epoch: 1, batch: 84/95, loss: 0.10654485002160072\n",
      "epoch: 1, batch: 85/95, loss: 0.10666140243411064\n",
      "epoch: 1, batch: 86/95, loss: 0.10492643415927887\n",
      "epoch: 1, batch: 87/95, loss: 0.1068454347550869\n",
      "epoch: 1, batch: 88/95, loss: 0.10549554899334908\n",
      "epoch: 1, batch: 89/95, loss: 0.1099846102297306\n",
      "epoch: 1, batch: 90/95, loss: 0.12469749301671981\n",
      "epoch: 1, batch: 91/95, loss: 0.11387416198849679\n",
      "epoch: 1, batch: 92/95, loss: 0.10786568820476532\n",
      "epoch: 1, batch: 93/95, loss: 0.10876466110348701\n",
      "epoch: 1, batch: 94/95, loss: 0.10802902504801751\n",
      "epoch: 1, batch: 94/95, loss: 0.11288177594542503\n",
      "Epoch train time: 1329.4565696716309 seconds\n",
      "\n",
      "epoch: 2, batch: 1/95, loss: 0.10100945085287094\n",
      "epoch: 2, batch: 2/95, loss: 0.10414600595831872\n",
      "epoch: 2, batch: 3/95, loss: 0.11269542574882507\n",
      "epoch: 2, batch: 4/95, loss: 0.10477589666843415\n",
      "epoch: 2, batch: 5/95, loss: 0.09791490212082862\n",
      "epoch: 2, batch: 6/95, loss: 0.10703627169132232\n",
      "epoch: 2, batch: 7/95, loss: 0.10563801303505897\n",
      "epoch: 2, batch: 8/95, loss: 0.10131562352180482\n",
      "epoch: 2, batch: 9/95, loss: 0.09885077849030495\n",
      "epoch: 2, batch: 10/95, loss: 0.0974675476551056\n",
      "epoch: 2, batch: 11/95, loss: 0.1142952062189579\n",
      "epoch: 2, batch: 12/95, loss: 0.10742210298776626\n",
      "epoch: 2, batch: 13/95, loss: 0.11383071541786194\n",
      "epoch: 2, batch: 14/95, loss: 0.1056182287633419\n",
      "epoch: 2, batch: 15/95, loss: 0.10001114681363106\n",
      "epoch: 2, batch: 16/95, loss: 0.10409783124923706\n",
      "epoch: 2, batch: 17/95, loss: 0.11043448895215988\n",
      "epoch: 2, batch: 18/95, loss: 0.10420330688357353\n",
      "epoch: 2, batch: 19/95, loss: 0.1052087590098381\n",
      "epoch: 2, batch: 20/95, loss: 0.09540729969739914\n",
      "epoch: 2, batch: 21/95, loss: 0.1045568861067295\n",
      "epoch: 2, batch: 22/95, loss: 0.10513046234846116\n",
      "epoch: 2, batch: 23/95, loss: 0.1061767429113388\n",
      "epoch: 2, batch: 24/95, loss: 0.11209985539317131\n",
      "epoch: 2, batch: 25/95, loss: 0.10372768640518189\n",
      "epoch: 2, batch: 26/95, loss: 0.09707476571202278\n",
      "epoch: 2, batch: 27/95, loss: 0.10622891336679459\n",
      "epoch: 2, batch: 28/95, loss: 0.10312954187393189\n",
      "epoch: 2, batch: 29/95, loss: 0.09860304817557335\n",
      "epoch: 2, batch: 30/95, loss: 0.10167834088206291\n",
      "epoch: 2, batch: 31/95, loss: 0.09770358055830002\n",
      "epoch: 2, batch: 32/95, loss: 0.11115444526076317\n",
      "epoch: 2, batch: 33/95, loss: 0.10081807225942611\n",
      "epoch: 2, batch: 34/95, loss: 0.1031282551586628\n",
      "epoch: 2, batch: 35/95, loss: 0.09496465921401978\n",
      "epoch: 2, batch: 36/95, loss: 0.11136934235692024\n",
      "epoch: 2, batch: 37/95, loss: 0.10132101774215699\n",
      "epoch: 2, batch: 38/95, loss: 0.10474388301372528\n",
      "epoch: 2, batch: 39/95, loss: 0.09409104660153389\n",
      "epoch: 2, batch: 40/95, loss: 0.10367884486913681\n",
      "epoch: 2, batch: 41/95, loss: 0.09743629917502403\n",
      "epoch: 2, batch: 42/95, loss: 0.11044437363743782\n",
      "epoch: 2, batch: 43/95, loss: 0.0957375593483448\n",
      "epoch: 2, batch: 44/95, loss: 0.09776496812701226\n",
      "epoch: 2, batch: 45/95, loss: 0.11092907711863517\n",
      "epoch: 2, batch: 46/95, loss: 0.1012846902012825\n",
      "epoch: 2, batch: 47/95, loss: 0.1076257161796093\n",
      "epoch: 2, batch: 48/95, loss: 0.10735701844096183\n",
      "epoch: 2, batch: 49/95, loss: 0.10742872804403306\n",
      "epoch: 2, batch: 50/95, loss: 0.09556457325816155\n",
      "epoch: 2, batch: 51/95, loss: 0.09564299508929253\n",
      "epoch: 2, batch: 52/95, loss: 0.1015313521027565\n",
      "epoch: 2, batch: 53/95, loss: 0.10017044395208359\n",
      "epoch: 2, batch: 54/95, loss: 0.09791643247008323\n",
      "epoch: 2, batch: 55/95, loss: 0.10732647404074669\n",
      "epoch: 2, batch: 56/95, loss: 0.09241263419389725\n",
      "epoch: 2, batch: 57/95, loss: 0.10215763822197914\n",
      "epoch: 2, batch: 58/95, loss: 0.09798648729920387\n",
      "epoch: 2, batch: 59/95, loss: 0.10352602377533912\n",
      "epoch: 2, batch: 60/95, loss: 0.09662141352891922\n",
      "epoch: 2, batch: 61/95, loss: 0.09666983187198638\n",
      "epoch: 2, batch: 62/95, loss: 0.10248787179589272\n",
      "epoch: 2, batch: 63/95, loss: 0.0954398050904274\n",
      "epoch: 2, batch: 64/95, loss: 0.10428128316998482\n",
      "epoch: 2, batch: 65/95, loss: 0.09854459948837757\n",
      "epoch: 2, batch: 66/95, loss: 0.10442626848816872\n",
      "epoch: 2, batch: 67/95, loss: 0.10256455168128013\n",
      "epoch: 2, batch: 68/95, loss: 0.10161524638533592\n",
      "epoch: 2, batch: 69/95, loss: 0.09882457256317138\n",
      "epoch: 2, batch: 70/95, loss: 0.09025645591318607\n",
      "epoch: 2, batch: 71/95, loss: 0.09904239475727081\n",
      "epoch: 2, batch: 72/95, loss: 0.10320784300565719\n",
      "epoch: 2, batch: 73/95, loss: 0.08989358171820641\n",
      "epoch: 2, batch: 74/95, loss: 0.09551672637462616\n",
      "epoch: 2, batch: 75/95, loss: 0.09361838027834893\n",
      "epoch: 2, batch: 76/95, loss: 0.09819694608449936\n",
      "epoch: 2, batch: 77/95, loss: 0.09949417039752007\n",
      "epoch: 2, batch: 78/95, loss: 0.093609519302845\n",
      "epoch: 2, batch: 79/95, loss: 0.10284533202648163\n",
      "epoch: 2, batch: 80/95, loss: 0.09838397055864334\n",
      "epoch: 2, batch: 81/95, loss: 0.08992581591010093\n",
      "epoch: 2, batch: 82/95, loss: 0.091907100379467\n",
      "epoch: 2, batch: 83/95, loss: 0.10144404992461205\n",
      "epoch: 2, batch: 84/95, loss: 0.09504974409937858\n",
      "epoch: 2, batch: 85/95, loss: 0.10052544996142387\n",
      "epoch: 2, batch: 86/95, loss: 0.0992596186697483\n",
      "epoch: 2, batch: 87/95, loss: 0.08561822324991226\n",
      "epoch: 2, batch: 88/95, loss: 0.08509106747806072\n",
      "epoch: 2, batch: 89/95, loss: 0.09354773461818695\n",
      "epoch: 2, batch: 90/95, loss: 0.09913174882531166\n",
      "epoch: 2, batch: 91/95, loss: 0.09805206283926964\n",
      "epoch: 2, batch: 92/95, loss: 0.09741578921675682\n",
      "epoch: 2, batch: 93/95, loss: 0.09835728630423546\n",
      "epoch: 2, batch: 94/95, loss: 0.10346545651555061\n",
      "epoch: 2, batch: 94/95, loss: 0.10232787057757378\n",
      "Epoch train time: 1335.917323589325 seconds\n",
      "\n",
      "Validating-> mb: 0, mode score: 0.04510605931282043, gloss score: 0.28627040386199953, timing score: 0.04108009338378906\n",
      "Validating-> mb: 1, mode score: 0.046861024200916285, gloss score: 0.26302907466888426, timing score: 0.047677659988403315\n",
      "Validating-> mb: 2, mode score: 0.047105487187703445, gloss score: 0.2453460693359375, timing score: 0.04522719979286194\n",
      "Validating-> mb: 3, mode score: 0.04462416619062423, gloss score: 0.24873126149177552, timing score: 0.04093968942761421\n",
      "Validating-> mb: 4, mode score: 0.04404923379421234, gloss score: 0.2540543031692505, timing score: 0.04439446747303009\n",
      "Validating-> mb: 5, mode score: 0.044002445042133326, gloss score: 0.26255673567454024, timing score: 0.04482631584008534\n",
      "Validating-> mb: 6, mode score: 0.044429358839988706, gloss score: 0.26225824696677075, timing score: 0.042769293274198254\n",
      "Validating-> mb: 7, mode score: 0.045183015987277024, gloss score: 0.2656430721282959, timing score: 0.041986643150448796\n",
      "Validating-> mb: 8, mode score: 0.04518441557884215, gloss score: 0.2634092224968805, timing score: 0.04111089938216739\n",
      "Validating-> mb: 9, mode score: 0.04471401304006576, gloss score: 0.254407948255539, timing score: 0.041411640942096706\n",
      "Validating-> mb: 10, mode score: 0.04436878155578266, gloss score: 0.25368826064196504, timing score: 0.041557903181422835\n",
      "Validating-> mb: 11, mode score: 0.04370543261369069, gloss score: 0.26290781199932106, timing score: 0.041156256447235744\n",
      "Validating-> mb: 12, mode score: 0.04374838815285609, gloss score: 0.2653474138333248, timing score: 0.041397109169226426\n",
      "Validating-> mb: 13, mode score: 0.04368383756705692, gloss score: 0.2584832199982235, timing score: 0.04099198281764984\n",
      "Validating-> mb: 14, mode score: 0.0438561862707138, gloss score: 0.2682024614016215, timing score: 0.041238314509391784\n",
      "Validating-> mb: 15, mode score: 0.044087926670908925, gloss score: 0.2671848259866238, timing score: 0.04210514966398478\n",
      "Validating-> mb: 16, mode score: 0.044090040466364694, gloss score: 0.27423259160097907, timing score: 0.0424740707173067\n",
      "Validating-> mb: 17, mode score: 0.04429673055807749, gloss score: 0.2746654914485084, timing score: 0.04371989965438843\n",
      "Validating-> mb: 18, mode score: 0.044370628344385245, gloss score: 0.2710720997107656, timing score: 0.04404002694707168\n",
      "Validating-> mb: 19, mode score: 0.04422148495912552, gloss score: 0.26885073840618134, timing score: 0.0434482242166996\n",
      "Validating-> mb: 20, mode score: 0.04452389024552845, gloss score: 0.2670153748421442, timing score: 0.043067187212762384\n",
      "Validating-> mb: 21, mode score: 0.04469302012161775, gloss score: 0.2689062243158167, timing score: 0.0434491446072405\n",
      "Validating-> mb: 22, mode score: 0.04471037517423215, gloss score: 0.27160931100016056, timing score: 0.0434031921884288\n",
      "Validating-> mb: 23, mode score: 0.04500775411725044, gloss score: 0.27050083527962365, timing score: 0.04381843457619349\n",
      "Validating-> mb: 24, mode score: 0.04507994556427002, gloss score: 0.2675702357292175, timing score: 0.044350302457809444\n",
      "Validating-> mb: 25, mode score: 0.04502925528929784, gloss score: 0.2680283706921797, timing score: 0.043836928674807915\n",
      "Validating-> mb: 26, mode score: 0.04487697062668977, gloss score: 0.26837302711274885, timing score: 0.04401850921136362\n",
      "Validating-> mb: 27, mode score: 0.044963913517338895, gloss score: 0.2707090228796005, timing score: 0.04369041046925953\n",
      "Validating-> mb: 28, mode score: 0.04527175981422951, gloss score: 0.2706068051272425, timing score: 0.043978886563202424\n",
      "Validating-> mb: 29, mode score: 0.045257180134455365, gloss score: 0.2709205122788747, timing score: 0.04364930858214696\n",
      "Validating-> mb: 30, mode score: 0.04543582700913953, gloss score: 0.2712779448878381, timing score: 0.04403910319651327\n",
      "Validating-> mb: 31, mode score: 0.04518090263009072, gloss score: 0.27008492313325405, timing score: 0.04489467302337289\n",
      "Validating-> mb: 32, mode score: 0.04520309603575505, gloss score: 0.2686046394434842, timing score: 0.04477886268586823\n",
      "Validating-> mb: 33, mode score: 0.04513472520253238, gloss score: 0.26700537660542656, timing score: 0.04469366310273899\n",
      "Validating-> mb: 34, mode score: 0.04485914843423026, gloss score: 0.26635954414095203, timing score: 0.045085702027593336\n",
      "Validating-> mb: 35, mode score: 0.04516961458656523, gloss score: 0.2659968657626046, timing score: 0.04485807319482168\n",
      "Validating-> mb: 36, mode score: 0.04523973795207771, gloss score: 0.26476559606758326, timing score: 0.044725216079402615\n",
      "Validating-> mb: 37, mode score: 0.04521254379498331, gloss score: 0.2634686479442998, timing score: 0.04459811213769411\n",
      "Validating-> mb: 38, mode score: 0.045192751594078845, gloss score: 0.26235466095117427, timing score: 0.04498500503026522\n",
      "Validating-> mb: 39, mode score: 0.0454018772393465, gloss score: 0.2601759243011475, timing score: 0.044999980032444\n",
      "Validating-> mb: 40, mode score: 0.04521836522148876, gloss score: 0.2609938673856782, timing score: 0.0449145281460227\n",
      "Validating-> mb: 41, mode score: 0.04519447208870024, gloss score: 0.2611935797191802, timing score: 0.04507892394349689\n",
      "Validating-> mb: 42, mode score: 0.04530338672704474, gloss score: 0.2633700664653335, timing score: 0.044927616105523226\n",
      "Validating-> mb: 43, mode score: 0.04531589705835688, gloss score: 0.262315705689517, timing score: 0.04571821723471989\n",
      "Validating-> mb: 44, mode score: 0.04549007203843857, gloss score: 0.261273979081048, timing score: 0.045560502343707614\n",
      "Validating-> mb: 45, mode score: 0.045595978394798606, gloss score: 0.2613419024840646, timing score: 0.045278062703816783\n",
      "Validating-> mb: 46, mode score: 0.04549427514380597, gloss score: 0.2611469152125907, timing score: 0.0454987218405338\n",
      "Validating-> mb: 47, mode score: 0.045722460622588786, gloss score: 0.2603396167357763, timing score: 0.04550396551688512\n",
      "Validating-> mb: 48, mode score: 0.04569122462856526, gloss score: 0.2597444899228155, timing score: 0.045289670082987565\n",
      "Validating-> mb: 49, mode score: 0.045879032611846914, gloss score: 0.25893372631073003, timing score: 0.04560567843914032\n",
      "Validating-> mb: 50, mode score: 0.04625113723324793, gloss score: 0.25988872471977686, timing score: 0.04584342276348787\n",
      "Validating-> mb: 51, mode score: 0.046246991019982545, gloss score: 0.2593761971363655, timing score: 0.04561951911220183\n",
      "Validating-> mb: 52, mode score: 0.04623670476787494, gloss score: 0.2599625164607786, timing score: 0.045672799112661824\n",
      "Validating-> mb: 53, mode score: 0.04626175844007067, gloss score: 0.2603630997516491, timing score: 0.04540946257335168\n",
      "Validating-> mb: 54, mode score: 0.046142747998237595, gloss score: 0.2611777409640226, timing score: 0.045502824187278745\n",
      "Validating-> mb: 55, mode score: 0.046129445678421414, gloss score: 0.2618450437273298, timing score: 0.045517729701740395\n",
      "Validating-> mb: 56, mode score: 0.04610124549321959, gloss score: 0.2613695395620246, timing score: 0.045855305644503803\n",
      "Validating-> mb: 57, mode score: 0.04597006491545971, gloss score: 0.26059421876381184, timing score: 0.04577212554627451\n",
      "Validating-> mb: 58, mode score: 0.04604601769124045, gloss score: 0.26067146689204845, timing score: 0.04583097443742267\n",
      "Validating-> mb: 59, mode score: 0.04599571118752159, gloss score: 0.2604943879445394, timing score: 0.04581750278671582\n",
      "Validating-> mb: 60, mode score: 0.04590415534425952, gloss score: 0.25965665324789583, timing score: 0.04590704993146365\n",
      "Validating-> mb: 61, mode score: 0.045926443559508145, gloss score: 0.2594787143891858, timing score: 0.04592279724536403\n",
      "Validating-> mb: 62, mode score: 0.045866512968426636, gloss score: 0.2592920958049714, timing score: 0.045810726947254604\n",
      "Validating-> mb: 63, mode score: 0.04608489656820891, gloss score: 0.26028023101389414, timing score: 0.04584303069859743\n",
      "Validating-> mb: 64, mode score: 0.04592930142696085, gloss score: 0.25966649679037246, timing score: 0.045946238407721884\n",
      "Validating-> mb: 65, mode score: 0.04587180199045121, gloss score: 0.2605172717210019, timing score: 0.04584460208813349\n",
      "Validating-> mb: 66, mode score: 0.045749742548857136, gloss score: 0.26111101178980595, timing score: 0.04569354052863904\n",
      "Validating-> mb: 67, mode score: 0.045825359619715615, gloss score: 0.26092543777297533, timing score: 0.045527547334923466\n",
      "Validating-> mb: 68, mode score: 0.04597942194213035, gloss score: 0.2608508693999139, timing score: 0.04540085589540178\n",
      "Validating-> mb: 69, mode score: 0.045968786605766816, gloss score: 0.26081251961844315, timing score: 0.045247702002525335\n",
      "Validating-> mb: 70, mode score: 0.04586536225298758, gloss score: 0.26066099455658825, timing score: 0.04536001497591046\n",
      "Validating-> mb: 71, mode score: 0.045875834342506173, gloss score: 0.2605490204360751, timing score: 0.045449368572897386\n",
      "Validating-> mb: 72, mode score: 0.0457685154186536, gloss score: 0.26011460154023897, timing score: 0.045519767722038376\n",
      "Validating-> mb: 73, mode score: 0.045806426816695424, gloss score: 0.26041231928644964, timing score: 0.04550788942221049\n",
      "Validating-> mb: 74, mode score: 0.04575962698459623, gloss score: 0.2594523744583131, timing score: 0.04543478528658549\n",
      "Validating-> mb: 75, mode score: 0.04576498234742564, gloss score: 0.2597740706644561, timing score: 0.04534639586743555\n",
      "Validating-> mb: 76, mode score: 0.045775914579242835, gloss score: 0.26039305816997194, timing score: 0.045377880102628235\n",
      "Validating-> mb: 77, mode score: 0.045635325633562505, gloss score: 0.25976915114965204, timing score: 0.04526028877649551\n",
      "Validating-> mb: 78, mode score: 0.04564915801150887, gloss score: 0.2595603146130527, timing score: 0.04520821239374861\n",
      "Validating-> mb: 79, mode score: 0.04571366038173435, gloss score: 0.2605274513363839, timing score: 0.0450117802247405\n",
      "Validating-> mb: 80, mode score: 0.045795785608114994, gloss score: 0.2606382549544912, timing score: 0.04494329179510658\n",
      "Validating-> mb: 81, mode score: 0.04570433577386342, gloss score: 0.26017569710568694, timing score: 0.04480063402798117\n",
      "Validating-> mb: 82, mode score: 0.045563518318785225, gloss score: 0.25975594807820157, timing score: 0.04480526874582451\n",
      "Validating-> mb: 83, mode score: 0.045504737929219274, gloss score: 0.2596531916232337, timing score: 0.04478624172153926\n",
      "Validating-> mb: 84, mode score: 0.04541408065487355, gloss score: 0.2588968233501211, timing score: 0.044828649548923266\n",
      "Validating-> mb: 85, mode score: 0.045326004783774514, gloss score: 0.25876375173413485, timing score: 0.044807192198065816\n",
      "Validating-> mb: 86, mode score: 0.04541198511918384, gloss score: 0.25882223433461693, timing score: 0.044747332423582836\n",
      "Validating-> mb: 87, mode score: 0.045568505166606446, gloss score: 0.25903503203933903, timing score: 0.04450665060430765\n",
      "Validating-> mb: 88, mode score: 0.04553548077519018, gloss score: 0.2585710382193663, timing score: 0.04498743781547867\n",
      "Validating-> mb: 89, mode score: 0.045517435901694804, gloss score: 0.25932694713274645, timing score: 0.04486162333024872\n",
      "Validating-> mb: 90, mode score: 0.04547430450444691, gloss score: 0.25883170261487864, timing score: 0.044817359581753445\n",
      "Validating-> mb: 91, mode score: 0.04557572612943855, gloss score: 0.25910787750845377, timing score: 0.04500530085161975\n",
      "Validating-> mb: 92, mode score: 0.0455847241545236, gloss score: 0.25918540121406647, timing score: 0.04486823054731532\n",
      "Validating-> mb: 93, mode score: 0.04558394034492207, gloss score: 0.26049324286744957, timing score: 0.04505427490840566\n",
      "Validating-> mb: 94, mode score: 0.04561849468632746, gloss score: 0.26009063457187864, timing score: 0.04493154273221367\n",
      "Validating-> mb: 95, mode score: 0.04566780359794695, gloss score: 0.2598494059095781, timing score: 0.04492951676559945\n",
      "Validating-> mb: 96, mode score: 0.0457634389400482, gloss score: 0.2602429165053614, timing score: 0.04494766428605797\n",
      "Validating-> mb: 97, mode score: 0.04573164326804023, gloss score: 0.26013121203500406, timing score: 0.04494693402429016\n",
      "Validating-> mb: 98, mode score: 0.04575869531342476, gloss score: 0.2602618057318409, timing score: 0.04490765849448213\n",
      "Validating-> mb: 99, mode score: 0.04574434772133826, gloss score: 0.2601239985227586, timing score: 0.044816243454813956\n",
      "Validating-> mb: 100, mode score: 0.04574300269089122, gloss score: 0.25955094901642006, timing score: 0.04490128287879547\n",
      "Validating-> mb: 101, mode score: 0.045758150021235136, gloss score: 0.2597671381398744, timing score: 0.04483505839518472\n",
      "Validating-> mb: 102, mode score: 0.04575272674120744, gloss score: 0.2592615267605458, timing score: 0.04474071230992531\n",
      "Validating-> mb: 103, mode score: 0.045686438335822165, gloss score: 0.2589919355053169, timing score: 0.04465783274231049\n",
      "Validating-> mb: 104, mode score: 0.04572432018461681, gloss score: 0.2589803526515053, timing score: 0.044663080275058754\n",
      "Validating-> mb: 105, mode score: 0.045683936915307664, gloss score: 0.2590443621266564, timing score: 0.04457400408837031\n",
      "Validating-> mb: 106, mode score: 0.045698469236632365, gloss score: 0.2593673475434848, timing score: 0.04471288118685518\n",
      "Validating-> mb: 107, mode score: 0.04567132363164866, gloss score: 0.25954198274347523, timing score: 0.04450839059772315\n",
      "Validating-> mb: 108, mode score: 0.045669399926421834, gloss score: 0.2599850561640679, timing score: 0.044414963689419114\n",
      "Validating-> mb: 109, mode score: 0.04567173903638666, gloss score: 0.2602646599032663, timing score: 0.0443873373486779\n",
      "Validating-> mb: 110, mode score: 0.04568563052662857, gloss score: 0.25980950443594314, timing score: 0.044700212736387517\n",
      "Validating-> mb: 111, mode score: 0.045614798979035434, gloss score: 0.25947909450956763, timing score: 0.04483736774751119\n",
      "Validating-> mb: 112, mode score: 0.04562763975257366, gloss score: 0.2588464052276274, timing score: 0.04500085294774148\n",
      "Validating-> mb: 113, mode score: 0.04559683554005203, gloss score: 0.2589173531323149, timing score: 0.04498842682754785\n",
      "Validating-> mb: 114, mode score: 0.04558891674746637, gloss score: 0.2582681295146113, timing score: 0.04499740517657736\n",
      "Validating-> mb: 115, mode score: 0.04553445241060749, gloss score: 0.25817784975314967, timing score: 0.04494809104964652\n",
      "Val loss: 0.09026613817777719, (mean of mode: 0.04553445241060749, gloss: 0.2581778497531496, timing: 0.04494809104964652, pointing: 0.011213553795947675)\n",
      "\n",
      "\n",
      "Saving new best model at epoch 2 with val loss 0.0903\n",
      "epoch: 3, batch: 1/95, loss: 0.09087370336055756\n",
      "epoch: 3, batch: 2/95, loss: 0.0889978926628828\n",
      "epoch: 3, batch: 3/95, loss: 0.08163540810346603\n",
      "epoch: 3, batch: 4/95, loss: 0.09848027154803277\n",
      "epoch: 3, batch: 5/95, loss: 0.1029022440314293\n",
      "epoch: 3, batch: 6/95, loss: 0.09194113090634345\n",
      "epoch: 3, batch: 7/95, loss: 0.08928613960742951\n",
      "epoch: 3, batch: 8/95, loss: 0.08781788423657418\n",
      "epoch: 3, batch: 9/95, loss: 0.09100143760442733\n",
      "epoch: 3, batch: 10/95, loss: 0.09153823181986809\n",
      "epoch: 3, batch: 11/95, loss: 0.09870656952261925\n",
      "epoch: 3, batch: 12/95, loss: 0.08435307517647743\n",
      "epoch: 3, batch: 13/95, loss: 0.0939820371568203\n",
      "epoch: 3, batch: 14/95, loss: 0.09803640395402909\n",
      "epoch: 3, batch: 15/95, loss: 0.10462357848882675\n",
      "epoch: 3, batch: 16/95, loss: 0.0925971269607544\n",
      "epoch: 3, batch: 17/95, loss: 0.09866953939199448\n",
      "epoch: 3, batch: 18/95, loss: 0.09132580608129501\n",
      "epoch: 3, batch: 19/95, loss: 0.09606216698884965\n",
      "epoch: 3, batch: 20/95, loss: 0.09166750386357307\n",
      "epoch: 3, batch: 21/95, loss: 0.10589420795440674\n",
      "epoch: 3, batch: 22/95, loss: 0.09116030260920524\n",
      "epoch: 3, batch: 23/95, loss: 0.09393149390816688\n",
      "epoch: 3, batch: 24/95, loss: 0.09286685883998871\n",
      "epoch: 3, batch: 25/95, loss: 0.08718974888324738\n",
      "epoch: 3, batch: 26/95, loss: 0.08954053446650505\n",
      "epoch: 3, batch: 27/95, loss: 0.0926979847252369\n",
      "epoch: 3, batch: 28/95, loss: 0.10050206296145917\n",
      "epoch: 3, batch: 29/95, loss: 0.09676559418439865\n",
      "epoch: 3, batch: 30/95, loss: 0.10114155039191246\n",
      "epoch: 3, batch: 31/95, loss: 0.09316661134362221\n",
      "epoch: 3, batch: 32/95, loss: 0.10043719932436942\n",
      "epoch: 3, batch: 33/95, loss: 0.09935653433203698\n",
      "epoch: 3, batch: 34/95, loss: 0.09866379648447036\n",
      "epoch: 3, batch: 35/95, loss: 0.08852501958608627\n",
      "epoch: 3, batch: 36/95, loss: 0.09799565374851227\n",
      "epoch: 3, batch: 37/95, loss: 0.0923224113881588\n",
      "epoch: 3, batch: 38/95, loss: 0.08877319544553756\n",
      "epoch: 3, batch: 39/95, loss: 0.09061534032225609\n",
      "epoch: 3, batch: 40/95, loss: 0.10307448953390122\n",
      "epoch: 3, batch: 41/95, loss: 0.09709971435368062\n",
      "epoch: 3, batch: 42/95, loss: 0.08345240540802479\n",
      "epoch: 3, batch: 43/95, loss: 0.09654048979282379\n",
      "epoch: 3, batch: 44/95, loss: 0.10114647448062897\n",
      "epoch: 3, batch: 45/95, loss: 0.0881427526473999\n",
      "epoch: 3, batch: 46/95, loss: 0.09098831564188004\n",
      "epoch: 3, batch: 47/95, loss: 0.10222146511077881\n",
      "epoch: 3, batch: 48/95, loss: 0.09366223514080048\n",
      "epoch: 3, batch: 49/95, loss: 0.089170540869236\n",
      "epoch: 3, batch: 50/95, loss: 0.09072679653763771\n",
      "epoch: 3, batch: 51/95, loss: 0.09107274636626243\n",
      "epoch: 3, batch: 52/95, loss: 0.09702951163053512\n",
      "epoch: 3, batch: 53/95, loss: 0.07932742238044739\n",
      "epoch: 3, batch: 54/95, loss: 0.08774157054722309\n",
      "epoch: 3, batch: 55/95, loss: 0.0919793576002121\n",
      "epoch: 3, batch: 56/95, loss: 0.0997524157166481\n",
      "epoch: 3, batch: 57/95, loss: 0.07939798720180988\n",
      "epoch: 3, batch: 58/95, loss: 0.10010593682527542\n",
      "epoch: 3, batch: 59/95, loss: 0.0972578063607216\n",
      "epoch: 3, batch: 60/95, loss: 0.09959910362958908\n",
      "epoch: 3, batch: 61/95, loss: 0.09389321953058243\n",
      "epoch: 3, batch: 62/95, loss: 0.09434749782085419\n",
      "epoch: 3, batch: 63/95, loss: 0.09619608819484711\n",
      "epoch: 3, batch: 64/95, loss: 0.09684751182794571\n",
      "epoch: 3, batch: 65/95, loss: 0.09027828574180603\n",
      "epoch: 3, batch: 66/95, loss: 0.09898670390248299\n",
      "epoch: 3, batch: 67/95, loss: 0.09218834564089776\n",
      "epoch: 3, batch: 68/95, loss: 0.09325416758656502\n",
      "epoch: 3, batch: 69/95, loss: 0.08853014297783375\n",
      "epoch: 3, batch: 70/95, loss: 0.0874787837266922\n",
      "epoch: 3, batch: 71/95, loss: 0.09520040303468705\n",
      "epoch: 3, batch: 72/95, loss: 0.09465743824839593\n",
      "epoch: 3, batch: 73/95, loss: 0.08184832781553268\n",
      "epoch: 3, batch: 74/95, loss: 0.08806721121072769\n",
      "epoch: 3, batch: 75/95, loss: 0.08684437051415443\n",
      "epoch: 3, batch: 76/95, loss: 0.08930647075176239\n",
      "epoch: 3, batch: 77/95, loss: 0.08843142688274383\n",
      "epoch: 3, batch: 78/95, loss: 0.09808330610394478\n",
      "epoch: 3, batch: 79/95, loss: 0.0942260518670082\n",
      "epoch: 3, batch: 80/95, loss: 0.09567136242985726\n",
      "epoch: 3, batch: 81/95, loss: 0.08945121429860592\n",
      "epoch: 3, batch: 82/95, loss: 0.08318536654114723\n",
      "epoch: 3, batch: 83/95, loss: 0.09301396831870079\n",
      "epoch: 3, batch: 84/95, loss: 0.09332462027668953\n",
      "epoch: 3, batch: 85/95, loss: 0.09031873419880868\n",
      "epoch: 3, batch: 86/95, loss: 0.08532287403941155\n",
      "epoch: 3, batch: 87/95, loss: 0.07921656519174576\n",
      "epoch: 3, batch: 88/95, loss: 0.09235194846987724\n",
      "epoch: 3, batch: 89/95, loss: 0.08791045285761356\n",
      "epoch: 3, batch: 90/95, loss: 0.08468230441212654\n",
      "epoch: 3, batch: 91/95, loss: 0.08357018455863\n",
      "epoch: 3, batch: 92/95, loss: 0.09162897020578384\n",
      "epoch: 3, batch: 93/95, loss: 0.1032813958823681\n",
      "epoch: 3, batch: 94/95, loss: 0.0923054724931717\n",
      "epoch: 3, batch: 94/95, loss: 0.08970334455370903\n",
      "Epoch train time: 1280.9340121746063 seconds\n",
      "\n",
      "epoch: 4, batch: 1/95, loss: 0.08461524397134781\n",
      "epoch: 4, batch: 2/95, loss: 0.08907111883163452\n",
      "epoch: 4, batch: 3/95, loss: 0.08633108995854855\n",
      "epoch: 4, batch: 4/95, loss: 0.09308514147996902\n",
      "epoch: 4, batch: 5/95, loss: 0.09003745838999748\n",
      "epoch: 4, batch: 6/95, loss: 0.08421086072921753\n",
      "epoch: 4, batch: 7/95, loss: 0.08641651496291161\n",
      "epoch: 4, batch: 8/95, loss: 0.08519183471798897\n",
      "epoch: 4, batch: 9/95, loss: 0.0842701680958271\n",
      "epoch: 4, batch: 10/95, loss: 0.08982718735933304\n",
      "epoch: 4, batch: 11/95, loss: 0.0778878964483738\n",
      "epoch: 4, batch: 12/95, loss: 0.08758860379457474\n",
      "epoch: 4, batch: 13/95, loss: 0.08051675036549569\n",
      "epoch: 4, batch: 14/95, loss: 0.08721440583467484\n",
      "epoch: 4, batch: 15/95, loss: 0.07265071906149387\n",
      "epoch: 4, batch: 16/95, loss: 0.0857428602874279\n",
      "epoch: 4, batch: 17/95, loss: 0.08752066865563393\n",
      "epoch: 4, batch: 18/95, loss: 0.08304882124066353\n",
      "epoch: 4, batch: 19/95, loss: 0.09133113622665405\n",
      "epoch: 4, batch: 20/95, loss: 0.08896800056099892\n",
      "epoch: 4, batch: 21/95, loss: 0.07730416133999825\n",
      "epoch: 4, batch: 22/95, loss: 0.09321718290448189\n",
      "epoch: 4, batch: 23/95, loss: 0.08940170332789421\n",
      "epoch: 4, batch: 24/95, loss: 0.0839177630841732\n",
      "epoch: 4, batch: 25/95, loss: 0.09100879803299904\n",
      "epoch: 4, batch: 26/95, loss: 0.09707446321845055\n",
      "epoch: 4, batch: 27/95, loss: 0.07598075941205025\n",
      "epoch: 4, batch: 28/95, loss: 0.08699185028672218\n",
      "epoch: 4, batch: 29/95, loss: 0.08243731558322906\n",
      "epoch: 4, batch: 30/95, loss: 0.09503340348601341\n",
      "epoch: 4, batch: 31/95, loss: 0.08885687217116356\n",
      "epoch: 4, batch: 32/95, loss: 0.08191757909953594\n",
      "epoch: 4, batch: 33/95, loss: 0.08135713748633862\n",
      "epoch: 4, batch: 34/95, loss: 0.09526155889034271\n",
      "epoch: 4, batch: 35/95, loss: 0.085373779758811\n",
      "epoch: 4, batch: 36/95, loss: 0.0802184633910656\n",
      "epoch: 4, batch: 37/95, loss: 0.08266259208321572\n",
      "epoch: 4, batch: 38/95, loss: 0.08777455389499664\n",
      "epoch: 4, batch: 39/95, loss: 0.08640976697206497\n",
      "epoch: 4, batch: 40/95, loss: 0.08907234370708465\n",
      "epoch: 4, batch: 41/95, loss: 0.08836111091077328\n",
      "epoch: 4, batch: 42/95, loss: 0.08374031037092208\n",
      "epoch: 4, batch: 43/95, loss: 0.082609673589468\n",
      "epoch: 4, batch: 44/95, loss: 0.09846829324960708\n",
      "epoch: 4, batch: 45/95, loss: 0.08154574893414974\n",
      "epoch: 4, batch: 46/95, loss: 0.08750260472297669\n",
      "epoch: 4, batch: 47/95, loss: 0.09377874657511712\n",
      "epoch: 4, batch: 48/95, loss: 0.08902440518140793\n",
      "epoch: 4, batch: 49/95, loss: 0.08217869400978088\n",
      "epoch: 4, batch: 50/95, loss: 0.08017956838011742\n",
      "epoch: 4, batch: 51/95, loss: 0.08055279478430748\n",
      "epoch: 4, batch: 52/95, loss: 0.09096457064151764\n",
      "epoch: 4, batch: 53/95, loss: 0.09188250675797463\n",
      "epoch: 4, batch: 54/95, loss: 0.08471398949623107\n",
      "epoch: 4, batch: 55/95, loss: 0.08668386191129684\n",
      "epoch: 4, batch: 56/95, loss: 0.09111683703958988\n",
      "epoch: 4, batch: 57/95, loss: 0.0823035292327404\n",
      "epoch: 4, batch: 58/95, loss: 0.08627366721630096\n",
      "epoch: 4, batch: 59/95, loss: 0.08829213753342628\n",
      "epoch: 4, batch: 60/95, loss: 0.0904540080577135\n",
      "epoch: 4, batch: 61/95, loss: 0.09032550603151321\n",
      "epoch: 4, batch: 62/95, loss: 0.0776178389787674\n",
      "epoch: 4, batch: 63/95, loss: 0.08401478603482246\n",
      "epoch: 4, batch: 64/95, loss: 0.07583465240895748\n",
      "epoch: 4, batch: 65/95, loss: 0.08542246259748935\n",
      "epoch: 4, batch: 66/95, loss: 0.0926862470805645\n",
      "epoch: 4, batch: 67/95, loss: 0.08556263819336891\n",
      "epoch: 4, batch: 68/95, loss: 0.08101703375577926\n",
      "epoch: 4, batch: 69/95, loss: 0.09116333872079849\n",
      "epoch: 4, batch: 70/95, loss: 0.07934767678380013\n",
      "epoch: 4, batch: 71/95, loss: 0.07391093373298645\n",
      "epoch: 4, batch: 72/95, loss: 0.08352945707738399\n",
      "epoch: 4, batch: 73/95, loss: 0.09168000519275665\n",
      "epoch: 4, batch: 74/95, loss: 0.0813001524657011\n",
      "epoch: 4, batch: 75/95, loss: 0.08926463723182679\n",
      "epoch: 4, batch: 76/95, loss: 0.08855920732021332\n",
      "epoch: 4, batch: 77/95, loss: 0.08883263245224952\n",
      "epoch: 4, batch: 78/95, loss: 0.08950907215476037\n",
      "epoch: 4, batch: 79/95, loss: 0.08673802390694618\n",
      "epoch: 4, batch: 80/95, loss: 0.09085855185985565\n",
      "epoch: 4, batch: 81/95, loss: 0.0916402630507946\n",
      "epoch: 4, batch: 82/95, loss: 0.0830418661236763\n",
      "epoch: 4, batch: 83/95, loss: 0.08941119834780693\n",
      "epoch: 4, batch: 84/95, loss: 0.08482079580426216\n",
      "epoch: 4, batch: 85/95, loss: 0.08462003506720066\n",
      "epoch: 4, batch: 86/95, loss: 0.09994500875473022\n",
      "epoch: 4, batch: 87/95, loss: 0.08001734800636769\n",
      "epoch: 4, batch: 88/95, loss: 0.08146883621811866\n",
      "epoch: 4, batch: 89/95, loss: 0.08307998478412629\n",
      "epoch: 4, batch: 90/95, loss: 0.08623537421226501\n",
      "epoch: 4, batch: 91/95, loss: 0.0873559407889843\n",
      "epoch: 4, batch: 92/95, loss: 0.09334331154823303\n",
      "epoch: 4, batch: 93/95, loss: 0.09091735854744912\n",
      "epoch: 4, batch: 94/95, loss: 0.09221331849694252\n",
      "epoch: 4, batch: 94/95, loss: 0.08814462423324584\n",
      "Epoch train time: 1343.334459066391 seconds\n",
      "\n",
      "Validating-> mb: 0, mode score: 0.05223648548126221, gloss score: 0.22466204166412354, timing score: 0.06912748217582702\n",
      "Validating-> mb: 1, mode score: 0.0453375518321991, gloss score: 0.20924062728881837, timing score: 0.054365971684455866\n",
      "Validating-> mb: 2, mode score: 0.04360469977060954, gloss score: 0.17610780398050943, timing score: 0.05443279345830281\n",
      "Validating-> mb: 3, mode score: 0.04386084303259849, gloss score: 0.18521255254745483, timing score: 0.05657475292682647\n",
      "Validating-> mb: 4, mode score: 0.043033481836318964, gloss score: 0.18727636814117432, timing score: 0.05417744100093841\n",
      "Validating-> mb: 5, mode score: 0.04287955015897751, gloss score: 0.2000579595565796, timing score: 0.05418176501989364\n",
      "Validating-> mb: 6, mode score: 0.04198876065867287, gloss score: 0.21797465596880233, timing score: 0.05379740978990282\n",
      "Validating-> mb: 7, mode score: 0.041380416974425314, gloss score: 0.22336473464965823, timing score: 0.05158558860421181\n",
      "Validating-> mb: 8, mode score: 0.04174956248866187, gloss score: 0.22072534163792928, timing score: 0.05126544170909458\n",
      "Validating-> mb: 9, mode score: 0.042887258827686306, gloss score: 0.21902718186378478, timing score: 0.049066967368125915\n",
      "Validating-> mb: 10, mode score: 0.043106094002723694, gloss score: 0.21911606246774848, timing score: 0.04715355932712555\n",
      "Validating-> mb: 11, mode score: 0.043291851629813506, gloss score: 0.21812078058719633, timing score: 0.04557949751615523\n",
      "Validating-> mb: 12, mode score: 0.044409499030846816, gloss score: 0.22122182387572067, timing score: 0.045265474686255816\n",
      "Validating-> mb: 13, mode score: 0.043816170522144864, gloss score: 0.22109701888901845, timing score: 0.04474138383354459\n",
      "Validating-> mb: 14, mode score: 0.0430868516365687, gloss score: 0.22091769297917682, timing score: 0.044257963895797725\n",
      "Validating-> mb: 15, mode score: 0.04324448220431805, gloss score: 0.21858725175261495, timing score: 0.044458539597690104\n",
      "Validating-> mb: 16, mode score: 0.04314729413565468, gloss score: 0.21628609685336841, timing score: 0.04386602640151977\n",
      "Validating-> mb: 17, mode score: 0.04308242516385184, gloss score: 0.21413335601488748, timing score: 0.04387922651237911\n",
      "Validating-> mb: 18, mode score: 0.04335994140097969, gloss score: 0.21523217715715107, timing score: 0.043835949584057454\n",
      "Validating-> mb: 19, mode score: 0.04341088518500328, gloss score: 0.21788902103900906, timing score: 0.04286356151103973\n",
      "Validating-> mb: 20, mode score: 0.04306942621866862, gloss score: 0.21899834984824768, timing score: 0.042256938275836764\n",
      "Validating-> mb: 21, mode score: 0.04293797354806553, gloss score: 0.2194746705618771, timing score: 0.042121698504144496\n",
      "Validating-> mb: 22, mode score: 0.042525297662486194, gloss score: 0.219538532650989, timing score: 0.04161887907463571\n",
      "Validating-> mb: 23, mode score: 0.042621276155114164, gloss score: 0.21954912493626275, timing score: 0.041606448714931805\n",
      "Validating-> mb: 24, mode score: 0.04265921998023986, gloss score: 0.21891076803207393, timing score: 0.04111479294300079\n",
      "Validating-> mb: 25, mode score: 0.04263890832662582, gloss score: 0.2223989188671112, timing score: 0.0407453627540515\n",
      "Validating-> mb: 26, mode score: 0.04327982630994584, gloss score: 0.22609930435816444, timing score: 0.04074910040254947\n",
      "Validating-> mb: 27, mode score: 0.043266725114413665, gloss score: 0.22587998381682803, timing score: 0.04060994567615646\n",
      "Validating-> mb: 28, mode score: 0.043409892402846234, gloss score: 0.2244868356606056, timing score: 0.0402582564230623\n",
      "Validating-> mb: 29, mode score: 0.04301767845948536, gloss score: 0.2228425371646881, timing score: 0.03987687985102337\n",
      "Validating-> mb: 30, mode score: 0.04267907584867169, gloss score: 0.22359033053921112, timing score: 0.03952309133545046\n",
      "Validating-> mb: 31, mode score: 0.04274847991764545, gloss score: 0.2232517745345831, timing score: 0.03921050177887083\n",
      "Validating-> mb: 32, mode score: 0.042689362168312074, gloss score: 0.22197964588801064, timing score: 0.039147563775380465\n",
      "Validating-> mb: 33, mode score: 0.04284560680389404, gloss score: 0.22010489421732282, timing score: 0.03967931551091813\n",
      "Validating-> mb: 34, mode score: 0.04304854563304356, gloss score: 0.22123344421386715, timing score: 0.0402472598212106\n",
      "Validating-> mb: 35, mode score: 0.04312786261240641, gloss score: 0.2220269507831997, timing score: 0.04025582753949696\n",
      "Validating-> mb: 36, mode score: 0.043064431403134325, gloss score: 0.22536768719956682, timing score: 0.040301153869242295\n",
      "Validating-> mb: 37, mode score: 0.042823998159483866, gloss score: 0.2264304700650667, timing score: 0.04055642305236115\n",
      "Validating-> mb: 38, mode score: 0.04286848451846686, gloss score: 0.22756085762610803, timing score: 0.04096725178070559\n",
      "Validating-> mb: 39, mode score: 0.04298839099705219, gloss score: 0.22754413008689883, timing score: 0.04141760878264905\n",
      "Validating-> mb: 40, mode score: 0.04322723622729139, gloss score: 0.22763604536289125, timing score: 0.041529717096468306\n",
      "Validating-> mb: 41, mode score: 0.0433650098386265, gloss score: 0.22811976046789262, timing score: 0.04165084369125821\n",
      "Validating-> mb: 42, mode score: 0.043400053160135135, gloss score: 0.2270152250001597, timing score: 0.04186375175797663\n",
      "Validating-> mb: 43, mode score: 0.04333287138830532, gloss score: 0.22586369568651374, timing score: 0.04184878752990203\n",
      "Validating-> mb: 44, mode score: 0.043170791003439166, gloss score: 0.22639281749725343, timing score: 0.042053595781326295\n",
      "Validating-> mb: 45, mode score: 0.043203748373881634, gloss score: 0.22843179495438284, timing score: 0.041975921198077824\n",
      "Validating-> mb: 46, mode score: 0.04303291205396044, gloss score: 0.2284024015386054, timing score: 0.042444451375210544\n",
      "Validating-> mb: 47, mode score: 0.04297622746477525, gloss score: 0.22856497565905254, timing score: 0.04226659865429005\n",
      "Validating-> mb: 48, mode score: 0.04284901387837469, gloss score: 0.22778644707738138, timing score: 0.0420665129106872\n",
      "Validating-> mb: 49, mode score: 0.04278221076726914, gloss score: 0.22729169893264772, timing score: 0.04251743817329407\n",
      "Validating-> mb: 50, mode score: 0.04288957995526931, gloss score: 0.22766653556449742, timing score: 0.04270789892065759\n",
      "Validating-> mb: 51, mode score: 0.04280328956934122, gloss score: 0.22681688437095057, timing score: 0.04310292807909159\n",
      "Validating-> mb: 52, mode score: 0.042891640539439224, gloss score: 0.2271348309966753, timing score: 0.04305168311550933\n",
      "Validating-> mb: 53, mode score: 0.04308399144146177, gloss score: 0.22767645782894558, timing score: 0.04284577899509007\n",
      "Validating-> mb: 54, mode score: 0.04305389350110834, gloss score: 0.22663974068381568, timing score: 0.04274822321805088\n",
      "Validating-> mb: 55, mode score: 0.042948604002594946, gloss score: 0.22625529042312076, timing score: 0.042703773240957955\n",
      "Validating-> mb: 56, mode score: 0.04305075124690407, gloss score: 0.22715767057318434, timing score: 0.04273115223959874\n",
      "Validating-> mb: 57, mode score: 0.04316858397475604, gloss score: 0.22622596929813252, timing score: 0.04273911103092392\n",
      "Validating-> mb: 58, mode score: 0.043100634368799504, gloss score: 0.22678383972685212, timing score: 0.04269229057481734\n",
      "Validating-> mb: 59, mode score: 0.04288831646243731, gloss score: 0.2270040432612101, timing score: 0.04259820997714996\n",
      "Validating-> mb: 60, mode score: 0.042977820556671896, gloss score: 0.2266855388391213, timing score: 0.04248735792324192\n",
      "Validating-> mb: 61, mode score: 0.042851987769526825, gloss score: 0.22562067931698213, timing score: 0.04253977888053464\n",
      "Validating-> mb: 62, mode score: 0.04280616830265712, gloss score: 0.225307421835642, timing score: 0.04239270119439988\n",
      "Validating-> mb: 63, mode score: 0.04272833121940494, gloss score: 0.22589164450764654, timing score: 0.04247320112772286\n",
      "Validating-> mb: 64, mode score: 0.042748922063754156, gloss score: 0.22584246708796574, timing score: 0.043037579563947825\n",
      "Validating-> mb: 65, mode score: 0.04276004844542706, gloss score: 0.226625995202498, timing score: 0.042952046204697\n",
      "Validating-> mb: 66, mode score: 0.042625295804507704, gloss score: 0.22655338742839753, timing score: 0.0429393874175513\n",
      "Validating-> mb: 67, mode score: 0.04263374993906303, gloss score: 0.22702386835042165, timing score: 0.0428710071479573\n",
      "Validating-> mb: 68, mode score: 0.04261976864890777, gloss score: 0.22772553347159122, timing score: 0.042831324926321065\n",
      "Validating-> mb: 69, mode score: 0.04258254353489196, gloss score: 0.2283472657203674, timing score: 0.042907863599913464\n",
      "Validating-> mb: 70, mode score: 0.042511928291387974, gloss score: 0.22887347819099962, timing score: 0.042892767635869314\n",
      "Validating-> mb: 71, mode score: 0.04259432227247292, gloss score: 0.22876203325059677, timing score: 0.04278174945049816\n",
      "Validating-> mb: 72, mode score: 0.04264545126320566, gloss score: 0.22855627177512805, timing score: 0.04271546532846477\n",
      "Validating-> mb: 73, mode score: 0.04269294932081894, gloss score: 0.22948926912771686, timing score: 0.04271685150829521\n",
      "Validating-> mb: 74, mode score: 0.04261471207936606, gloss score: 0.22940648841857908, timing score: 0.04260104175408681\n",
      "Validating-> mb: 75, mode score: 0.04256616136745404, gloss score: 0.22881875147944997, timing score: 0.04261138725437617\n",
      "Validating-> mb: 76, mode score: 0.04258044923280742, gloss score: 0.22870011190315342, timing score: 0.04274776744378078\n",
      "Validating-> mb: 77, mode score: 0.04257213232608943, gloss score: 0.2291428643923539, timing score: 0.0426226875338799\n",
      "Validating-> mb: 78, mode score: 0.04249773663056049, gloss score: 0.22837371554555766, timing score: 0.042425243613086173\n",
      "Validating-> mb: 79, mode score: 0.04248900320380927, gloss score: 0.22835564434528344, timing score: 0.04280944973230363\n",
      "Validating-> mb: 80, mode score: 0.0425328737055814, gloss score: 0.22959077417114626, timing score: 0.042720291901517805\n",
      "Validating-> mb: 81, mode score: 0.04251312101032677, gloss score: 0.22877223956875678, timing score: 0.042680651930774145\n",
      "Validating-> mb: 82, mode score: 0.04262850568237076, gloss score: 0.22840156641351164, timing score: 0.04266805530312551\n",
      "Validating-> mb: 83, mode score: 0.042736063152551663, gloss score: 0.2290482430231003, timing score: 0.04297961780712719\n",
      "Validating-> mb: 84, mode score: 0.04277431137421553, gloss score: 0.22838763307122617, timing score: 0.04301657322575066\n",
      "Validating-> mb: 85, mode score: 0.0428181522460871, gloss score: 0.22812790635020228, timing score: 0.04308469395997914\n",
      "Validating-> mb: 86, mode score: 0.042702042302866096, gloss score: 0.22827404901899134, timing score: 0.04302375970900745\n",
      "Validating-> mb: 87, mode score: 0.04272266223349355, gloss score: 0.2305366533723744, timing score: 0.04290485825728288\n",
      "Validating-> mb: 88, mode score: 0.042795961592974295, gloss score: 0.2311572566461027, timing score: 0.04301087022497414\n",
      "Validating-> mb: 89, mode score: 0.04275043216016558, gloss score: 0.23129835724830625, timing score: 0.0429641295472781\n",
      "Validating-> mb: 90, mode score: 0.04278804934941806, gloss score: 0.23081142836874655, timing score: 0.042953976857793216\n",
      "Validating-> mb: 91, mode score: 0.0428596489455389, gloss score: 0.2313012370596761, timing score: 0.04291139617562296\n",
      "Validating-> mb: 92, mode score: 0.04291207197532859, gloss score: 0.23075346703170446, timing score: 0.04297764682641595\n",
      "Validating-> mb: 93, mode score: 0.042866594772389605, gloss score: 0.2302727068992371, timing score: 0.04296481736162878\n",
      "Validating-> mb: 94, mode score: 0.04284950780241114, gloss score: 0.22984412707780533, timing score: 0.04301196346157478\n",
      "Validating-> mb: 95, mode score: 0.04285721176614365, gloss score: 0.22972821903725463, timing score: 0.04305314502368373\n",
      "Validating-> mb: 96, mode score: 0.04284522210814289, gloss score: 0.22945298497209843, timing score: 0.043058708494471534\n",
      "Validating-> mb: 97, mode score: 0.04299960650351583, gloss score: 0.22934557187313934, timing score: 0.043267692260596236\n",
      "Validating-> mb: 98, mode score: 0.04299813834103671, gloss score: 0.22932108423926614, timing score: 0.04326480046065168\n",
      "Validating-> mb: 99, mode score: 0.04301848071813584, gloss score: 0.22986875736713408, timing score: 0.04313583824038507\n",
      "Validating-> mb: 100, mode score: 0.04299851484818035, gloss score: 0.22928197277654513, timing score: 0.04327874039069262\n",
      "Validating-> mb: 101, mode score: 0.0428801401573069, gloss score: 0.22905829057973973, timing score: 0.043185196612395504\n",
      "Validating-> mb: 102, mode score: 0.0428339160183101, gloss score: 0.22844088667804754, timing score: 0.043226748270895884\n",
      "Validating-> mb: 103, mode score: 0.04287382163680517, gloss score: 0.22811972762529667, timing score: 0.04310521214054182\n",
      "Validating-> mb: 104, mode score: 0.04284910718599956, gloss score: 0.227769843850817, timing score: 0.04306100842498598\n",
      "Validating-> mb: 105, mode score: 0.042851412718026144, gloss score: 0.22739915420424262, timing score: 0.0429136537718323\n",
      "Validating-> mb: 106, mode score: 0.04289078731960226, gloss score: 0.2274645693948336, timing score: 0.04292254999419239\n",
      "Validating-> mb: 107, mode score: 0.0428448786062223, gloss score: 0.22729178380083154, timing score: 0.04286050203221815\n",
      "Validating-> mb: 108, mode score: 0.04275815609945071, gloss score: 0.2267877951674505, timing score: 0.042816396918865525\n",
      "Validating-> mb: 109, mode score: 0.04272267130288211, gloss score: 0.22645558606494556, timing score: 0.04287381350994111\n",
      "Validating-> mb: 110, mode score: 0.04271039345242956, gloss score: 0.22627659756858068, timing score: 0.0432028830588401\n",
      "Validating-> mb: 111, mode score: 0.04274075198918582, gloss score: 0.22637240109699114, timing score: 0.04310305219675814\n",
      "Validating-> mb: 112, mode score: 0.04271110333172622, gloss score: 0.2267288382074474, timing score: 0.04300747300143791\n",
      "Validating-> mb: 113, mode score: 0.0426700775299156, gloss score: 0.226233588812644, timing score: 0.042987480027633806\n",
      "Validating-> mb: 114, mode score: 0.04272519764692888, gloss score: 0.22735365473705788, timing score: 0.04311188842939294\n",
      "Validating-> mb: 115, mode score: 0.04273815165305961, gloss score: 0.22728266181616943, timing score: 0.04300663959363411\n",
      "Val loss: 0.08065248921569759, (mean of mode: 0.0427381516530596, gloss: 0.22728266181616943, timing: 0.04300663959363411, pointing: 0.010343177277594807)\n",
      "\n",
      "\n",
      "Saving new best model at epoch 4 with val loss 0.0807\n",
      "epoch: 5, batch: 1/95, loss: 0.07998832762241363\n",
      "epoch: 5, batch: 2/95, loss: 0.0794173762202263\n",
      "epoch: 5, batch: 3/95, loss: 0.0831073947250843\n",
      "epoch: 5, batch: 4/95, loss: 0.08583275638520718\n",
      "epoch: 5, batch: 5/95, loss: 0.08849643021821976\n",
      "epoch: 5, batch: 6/95, loss: 0.10104761496186257\n",
      "epoch: 5, batch: 7/95, loss: 0.07387373223900795\n",
      "epoch: 5, batch: 8/95, loss: 0.08739383518695831\n",
      "epoch: 5, batch: 9/95, loss: 0.07734558396041394\n",
      "epoch: 5, batch: 10/95, loss: 0.0967836432158947\n",
      "epoch: 5, batch: 11/95, loss: 0.08514056913554668\n",
      "epoch: 5, batch: 12/95, loss: 0.07426808290183544\n",
      "epoch: 5, batch: 13/95, loss: 0.08422330394387245\n",
      "epoch: 5, batch: 14/95, loss: 0.08319065049290657\n",
      "epoch: 5, batch: 15/95, loss: 0.09032409861683846\n",
      "epoch: 5, batch: 16/95, loss: 0.08878920897841454\n",
      "epoch: 5, batch: 17/95, loss: 0.0832054365426302\n",
      "epoch: 5, batch: 18/95, loss: 0.08265747427940369\n",
      "epoch: 5, batch: 19/95, loss: 0.08536669313907623\n",
      "epoch: 5, batch: 20/95, loss: 0.08184973411262035\n",
      "epoch: 5, batch: 21/95, loss: 0.08992244377732277\n",
      "epoch: 5, batch: 22/95, loss: 0.08425349816679954\n",
      "epoch: 5, batch: 23/95, loss: 0.07531596645712853\n",
      "epoch: 5, batch: 24/95, loss: 0.08389991968870163\n",
      "epoch: 5, batch: 25/95, loss: 0.0819574274122715\n",
      "epoch: 5, batch: 26/95, loss: 0.08137879893183708\n",
      "epoch: 5, batch: 27/95, loss: 0.08362679332494735\n",
      "epoch: 5, batch: 28/95, loss: 0.08556411936879157\n",
      "epoch: 5, batch: 29/95, loss: 0.07934845834970475\n",
      "epoch: 5, batch: 30/95, loss: 0.08322495706379414\n",
      "epoch: 5, batch: 31/95, loss: 0.08709625564515591\n",
      "epoch: 5, batch: 32/95, loss: 0.08541173636913299\n",
      "epoch: 5, batch: 33/95, loss: 0.08244347497820854\n",
      "epoch: 5, batch: 34/95, loss: 0.06792785301804542\n",
      "epoch: 5, batch: 35/95, loss: 0.07910699211061001\n",
      "epoch: 5, batch: 36/95, loss: 0.08127878382802009\n",
      "epoch: 5, batch: 37/95, loss: 0.08283939212560654\n",
      "epoch: 5, batch: 38/95, loss: 0.09766521826386451\n",
      "epoch: 5, batch: 39/95, loss: 0.08629307523369789\n",
      "epoch: 5, batch: 40/95, loss: 0.0847891166806221\n",
      "epoch: 5, batch: 41/95, loss: 0.08102283403277397\n",
      "epoch: 5, batch: 42/95, loss: 0.07808153443038464\n",
      "epoch: 5, batch: 43/95, loss: 0.08093310743570328\n",
      "epoch: 5, batch: 44/95, loss: 0.07534310594201088\n",
      "epoch: 5, batch: 45/95, loss: 0.08508620671927929\n",
      "epoch: 5, batch: 46/95, loss: 0.07533430457115173\n",
      "epoch: 5, batch: 47/95, loss: 0.07550580687820911\n",
      "epoch: 5, batch: 48/95, loss: 0.08699116557836532\n",
      "epoch: 5, batch: 49/95, loss: 0.0911359317600727\n",
      "epoch: 5, batch: 50/95, loss: 0.08110472448170185\n",
      "epoch: 5, batch: 51/95, loss: 0.08510526828467846\n",
      "epoch: 5, batch: 52/95, loss: 0.08717404380440712\n",
      "epoch: 5, batch: 53/95, loss: 0.0816933274269104\n",
      "epoch: 5, batch: 54/95, loss: 0.08663582056760788\n",
      "epoch: 5, batch: 55/95, loss: 0.09112166874110698\n",
      "epoch: 5, batch: 56/95, loss: 0.08330697007477283\n",
      "epoch: 5, batch: 57/95, loss: 0.07761337906122208\n",
      "epoch: 5, batch: 58/95, loss: 0.07811322323977947\n",
      "epoch: 5, batch: 59/95, loss: 0.08341940715909005\n",
      "epoch: 5, batch: 60/95, loss: 0.07548206560313701\n",
      "epoch: 5, batch: 61/95, loss: 0.0781103577464819\n",
      "epoch: 5, batch: 62/95, loss: 0.08388801887631417\n",
      "epoch: 5, batch: 63/95, loss: 0.08441300690174103\n",
      "epoch: 5, batch: 64/95, loss: 0.08182813450694085\n",
      "epoch: 5, batch: 65/95, loss: 0.08211858719587325\n",
      "epoch: 5, batch: 66/95, loss: 0.084810059517622\n",
      "epoch: 5, batch: 67/95, loss: 0.0830076053738594\n",
      "epoch: 5, batch: 68/95, loss: 0.08181165382266045\n",
      "epoch: 5, batch: 69/95, loss: 0.08094874173402786\n",
      "epoch: 5, batch: 70/95, loss: 0.08507376909255981\n",
      "epoch: 5, batch: 71/95, loss: 0.08852639272809029\n",
      "epoch: 5, batch: 72/95, loss: 0.0817772749811411\n",
      "epoch: 5, batch: 73/95, loss: 0.0843285795301199\n",
      "epoch: 5, batch: 74/95, loss: 0.07818273305892945\n",
      "epoch: 5, batch: 75/95, loss: 0.08309621438384056\n",
      "epoch: 5, batch: 76/95, loss: 0.08039824515581132\n",
      "epoch: 5, batch: 77/95, loss: 0.07880756556987763\n",
      "epoch: 5, batch: 78/95, loss: 0.07597608752548694\n",
      "epoch: 5, batch: 79/95, loss: 0.0832852452993393\n",
      "epoch: 5, batch: 80/95, loss: 0.07761252783238888\n",
      "epoch: 5, batch: 81/95, loss: 0.08249231092631817\n",
      "epoch: 5, batch: 82/95, loss: 0.08122888952493668\n",
      "epoch: 5, batch: 83/95, loss: 0.07536383494734764\n",
      "epoch: 5, batch: 84/95, loss: 0.08108440600335598\n",
      "epoch: 5, batch: 85/95, loss: 0.08246807307004929\n",
      "epoch: 5, batch: 86/95, loss: 0.0832885205745697\n",
      "epoch: 5, batch: 87/95, loss: 0.074439537525177\n",
      "epoch: 5, batch: 88/95, loss: 0.070474511384964\n",
      "epoch: 5, batch: 89/95, loss: 0.0784247387200594\n",
      "epoch: 5, batch: 90/95, loss: 0.0852521862834692\n",
      "epoch: 5, batch: 91/95, loss: 0.07990080788731575\n",
      "epoch: 5, batch: 92/95, loss: 0.08223716169595718\n",
      "epoch: 5, batch: 93/95, loss: 0.07966832667589188\n",
      "epoch: 5, batch: 94/95, loss: 0.07669967897236347\n",
      "epoch: 5, batch: 94/95, loss: 0.08464140594005584\n",
      "Epoch train time: 1311.0938818454742 seconds\n",
      "\n",
      "\n",
      "Generating translations...\n",
      "Evaluating mb 1/108  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\NIA-1-69 재난안전정보 수어영상 데이터_AI 모델소스\\train.py\", line 370, in <module>\n",
      "    main()\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\NIA-1-69 재난안전정보 수어영상 데이터_AI 모델소스\\train.py\", line 307, in main\n",
      "    bleu_score.append(bleu_test(\n",
      "                      ^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\NIA-1-69 재난안전정보 수어영상 데이터_AI 모델소스\\utils.py\", line 215, in bleu_test\n",
      "    translation = model.batch_translate(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\NIA-1-69 재난안전정보 수어영상 데이터_AI 모델소스\\models.py\", line 1080, in batch_translate\n",
      "    model_output = self.model(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 939, in forward\n",
      "    outputs = block(\n",
      "              ^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 403, in forward\n",
      "    attn_output, self_attn_weights = self.attn(\n",
      "                                     ^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 348, in forward\n",
      "    attn_output = self.c_proj(attn_output)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py\", line 118, in forward\n",
      "    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 0 transpose_mat2 0 m 768 n 16 k 768 mat1_ld 768 mat2_ld 768 result_ld 768 abcType 0 computeType 68 scaleType 0\n"
     ]
    }
   ],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A94rxlp2mwa6"
   },
   "source": [
    "테스트용이라 config.json에서 epoch 60 -> 5로 조정. GPT 계열의 모델이 많은 데이터+충분한 epoch이 필요한데, 둘 다 부족해서 BLEU 점수가 낮은 듯.\n",
    "GPU 용량 부족으로 batch_size 전부 8 -> 4로 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11430,
     "status": "ok",
     "timestamp": 1747642490376,
     "user": {
      "displayName": "YEJI",
      "userId": "07260244821978580970"
     },
     "user_tz": -540
    },
    "id": "Ope1sXuyixwQ",
    "outputId": "2e4b077e-1c44-442b-e5ba-af759ad2ee18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed 13717\n",
      "\n",
      "\n",
      "Loading model weights...\n",
      "\n",
      "Generating translations...\n",
      "Evaluating mb 1/109  0\n",
      "Evaluating mb 2/109  5.0503643322907025e-232\n",
      "Evaluating mb 3/109  5.0503643322907025e-232\n",
      "Evaluating mb 4/109  4.56351932961745e-232\n",
      "Evaluating mb 5/109  4.699893587865584e-232\n",
      "Evaluating mb 6/109  4.776339689672537e-232\n",
      "Evaluating mb 7/109  4.903820654902516e-232\n",
      "Evaluating mb 8/109  4.9891364798118534e-232\n",
      "Evaluating mb 9/109  5.0503643322907025e-232\n",
      "Evaluating mb 10/109  5.0503643322907025e-232\n",
      "Evaluating mb 11/109  5.0077414134689805e-232\n",
      "Evaluating mb 12/109  4.792154329712147e-156\n",
      "Evaluating mb 13/109  5.652120713211699e-156\n",
      "Evaluating mb 14/109  5.534526107783258e-156\n",
      "Evaluating mb 15/109  5.333202804068676e-156\n",
      "Evaluating mb 16/109  6.340047130047875e-156\n",
      "Evaluating mb 17/109  6.234421502461107e-156\n",
      "Evaluating mb 18/109  6.137028753505359e-156\n",
      "Evaluating mb 19/109  6.07359896945442e-156\n",
      "Evaluating mb 20/109  6.357652888785711e-156\n",
      "Evaluating mb 21/109  6.295851792122561e-156\n",
      "Evaluating mb 22/109  6.213820956090684e-156\n",
      "Evaluating mb 23/109  6.115110712032715e-156\n",
      "Evaluating mb 24/109  6.06436268129109e-156\n",
      "Evaluating mb 25/109  6.315934508996764e-156\n",
      "Evaluating mb 26/109  6.246258089144748e-156\n",
      "Evaluating mb 27/109  6.124959977593292e-156\n",
      "Evaluating mb 28/109  6.082459093617709e-156\n",
      "Evaluating mb 29/109  5.972856258852119e-156\n",
      "Evaluating mb 30/109  5.902800905211058e-156\n",
      "Evaluating mb 31/109  5.852429447068024e-156\n",
      "Evaluating mb 32/109  5.804136033637225e-156\n",
      "Evaluating mb 33/109  5.772553888587583e-156\n",
      "Evaluating mb 34/109  5.727430654514298e-156\n",
      "Evaluating mb 35/109  5.68401666310256e-156\n",
      "Evaluating mb 36/109  5.61564659289745e-156\n",
      "Evaluating mb 37/109  5.563283095077057e-156\n",
      "Evaluating mb 38/109  5.538170842131075e-156\n",
      "Evaluating mb 39/109  5.513395251908821e-156\n",
      "Evaluating mb 40/109  5.488970578923307e-156\n",
      "Evaluating mb 41/109  5.453765096024458e-156\n",
      "Evaluating mb 42/109  5.430466939115487e-156\n",
      "Evaluating mb 43/109  5.407514080936228e-156\n",
      "Evaluating mb 44/109  5.596482309962649e-156\n",
      "Evaluating mb 45/109  5.573353590386906e-156\n",
      "Evaluating mb 46/109  5.550588003958524e-156\n",
      "Evaluating mb 47/109  5.725607434435848e-156\n",
      "Evaluating mb 48/109  5.664369230606508e-156\n",
      "Evaluating mb 49/109  5.633435760972001e-156\n",
      "Evaluating mb 50/109  5.789541420923353e-156\n",
      "Evaluating mb 51/109  5.758886913808917e-156\n",
      "Evaluating mb 52/109  5.74673398140376e-156\n",
      "Evaluating mb 53/109  5.725807286113917e-156\n",
      "Evaluating mb 54/109  5.696834153338018e-156\n",
      "Evaluating mb 55/109  5.668575387172075e-156\n",
      "Evaluating mb 56/109  5.648995811520904e-156\n",
      "Evaluating mb 57/109  5.62190359044924e-156\n",
      "Evaluating mb 58/109  5.760420108936694e-156\n",
      "Evaluating mb 59/109  5.741166046940623e-156\n",
      "Evaluating mb 60/109  5.707303784728877e-156\n",
      "Evaluating mb 61/109  5.68163670736366e-156\n",
      "Evaluating mb 62/109  5.656532575812901e-156\n",
      "Evaluating mb 63/109  5.617836640734018e-156\n",
      "Evaluating mb 64/109  5.60100836343045e-156\n",
      "Evaluating mb 65/109  5.58438653971535e-156\n",
      "Evaluating mb 66/109  5.561328470794309e-156\n",
      "Evaluating mb 67/109  5.545255823849223e-156\n",
      "Evaluating mb 68/109  5.66922308456109e-156\n",
      "Evaluating mb 69/109  5.653021172859952e-156\n",
      "Evaluating mb 70/109  5.630771567386794e-156\n",
      "Evaluating mb 71/109  5.608949328040334e-156\n",
      "Evaluating mb 72/109  5.587540003269984e-156\n",
      "Evaluating mb 73/109  5.701061389682218e-156\n",
      "Evaluating mb 74/109  5.685718387957793e-156\n",
      "Evaluating mb 75/109  5.670554929954946e-156\n",
      "Evaluating mb 76/109  5.775687625902618e-156\n",
      "Evaluating mb 77/109  5.749050530428721e-156\n",
      "Evaluating mb 78/109  5.84716421781509e-156\n",
      "Evaluating mb 79/109  5.832123313888357e-156\n",
      "Evaluating mb 80/109  5.817254959199364e-156\n",
      "Evaluating mb 81/109  5.80255636027382e-156\n",
      "Evaluating mb 82/109  5.793337300153666e-156\n",
      "Evaluating mb 83/109  5.778880029260399e-156\n",
      "Evaluating mb 84/109  5.764586976439092e-156\n",
      "Evaluating mb 85/109  5.730171224215494e-156\n",
      "Evaluating mb 86/109  5.716529047054092e-156\n",
      "Evaluating mb 87/109  5.802253555866143e-156\n",
      "Evaluating mb 88/109  5.788669400416179e-156\n",
      "Evaluating mb 89/109  5.765482207227581e-156\n",
      "Evaluating mb 90/109  5.737852669893133e-156\n",
      "Evaluating mb 91/109  5.720289525658979e-156\n",
      "Evaluating mb 92/109  5.707703824167772e-156\n",
      "Evaluating mb 93/109  5.792584990969922e-156\n",
      "Evaluating mb 94/109  5.775336216697629e-156\n",
      "Evaluating mb 95/109  5.762916948296538e-156\n",
      "Evaluating mb 96/109  5.750617972326519e-156\n",
      "Evaluating mb 97/109  5.738437597134343e-156\n",
      "Evaluating mb 98/109  5.809375070891777e-156\n",
      "Evaluating mb 99/109  5.801694340417967e-156\n",
      "Evaluating mb 100/109  5.781018310584234e-156\n",
      "Evaluating mb 101/109  5.769236720541194e-156\n",
      "Evaluating mb 102/109  5.757564232342265e-156\n",
      "Evaluating mb 103/109  5.832915952041944e-156\n",
      "Evaluating mb 104/109  5.821235625560158e-156\n",
      "Evaluating mb 105/109  5.80555557096701e-156\n",
      "Evaluating mb 106/109  5.872992662536864e-156\n",
      "Evaluating mb 107/109  5.857465594463415e-156\n",
      "Evaluating mb 108/109  5.842138130978849e-156\n",
      "Evaluating mb 109/109  5.815028294996685e-156\n",
      "\n",
      "\n",
      "BLEU score pre train: 0\n",
      "BLEU score post train: 5.877799068880829e-156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "!python test.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8B_ILUF5izIj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered input: 1층에 불이 났으니 대피하세요.\n",
      "\n",
      "Loading model weights...\n",
      "\n",
      "Translating...\n",
      "\n",
      "Saving translations at: results/translate_sentence_20250609141608_*.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "!python translate_sentence.py \"1층에 불이 났으니 대피하세요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3JzCJlNi0Jx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered input path: results\n",
      "\n",
      "Loading model weights...\n",
      "\n",
      "Translating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\NIA-1-69 재난안전정보 수어영상 데이터_AI 모델소스\\translate_file.py\", line 177, in <module>\n",
      "    translate(timestamp, path, **parameters)\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\NIA-1-69 재난안전정보 수어영상 데이터_AI 모델소스\\translate_file.py\", line 96, in translate\n",
      "    dataset_test = JsonTextDataset(path)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\이예지학부휴학통계학과\\Downloads\\conference\\.venv\\NIA-1-69 재난안전정보 수어영상 데이터_AI 모델소스\\translate_file.py\", line 49, in __init__\n",
      "    self.sentences.append(json.load(f)['korean_text'])\n",
      "                          ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\이예지학부휴학통계학과\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py\", line 293, in load\n",
      "    return loads(fp.read(),\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\이예지학부휴학통계학과\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\이예지학부휴학통계학과\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\이예지학부휴학통계학과\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "!python translate_file.py  --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 수: 3797\n",
      "첫 번째 데이터 샘플:\n",
      "{'source_text': '03시 39분 현재 관악구 신림동 한신아파트 상가 화재발생으로 일부 도로가 혼잡하오니 안전에 유의하시기 바랍니다.', 'gloss_list': ['오늘1', '관악구', '신림', '장소1', '새벽1', '시:3시39분', '한신', '아파트1', '시장1', '불타다1', '사고1', '길1', '자동차밀리다1', '조심1'], 'source_tokens': [10595, 394, 7888, 14317, 7671, 9490, 31918, 6919, 36694, 7244, 9036, 7890, 47670, 37858, 25185, 21604, 9021, 9616, 19240, 9749, 8165, 8702, 8052, 7172, 27745, 25167, 23395, 9117, 7377, 12521], 'predict_from': 30, 'types': [0, 8, 2, 8, 6, 8, 6, 8, 3, 8, 3, 8, 2, 8, 6, 8, 2, 8, 2, 8, 2, 8, 2, 8, 2, 8, 2, 8, 2, 1], 'tokens': [np.int64(2), np.int64(4), np.int64(975), np.int64(4), 0, np.int64(4), 0, np.int64(4), np.int64(1055), np.int64(4), np.int64(402), np.int64(4), np.int64(633), np.int64(4), 0, np.int64(4), np.int64(925), np.int64(4), np.int64(906), np.int64(4), np.int64(369), np.int64(4), np.int64(384), np.int64(4), np.int64(98), np.int64(4), np.int64(1046), np.int64(4), np.int64(1076), np.int64(3)], 'signs': (np.int64(975), np.int64(1055), np.int64(402), np.int64(633), np.int64(925), np.int64(906), np.int64(369), np.int64(384), np.int64(98), np.int64(1046), np.int64(1076)), 'sign_inds': (2, 8, 10, 12, 16, 18, 20, 22, 24, 26, 28), 'nmss': [], 'nms_inds': [], 'token_durations': (0.40200000000000014, 0.35499999999999954, 0.23199999999999932, 1.1579999999999995, 0.40000000000000036, 0.32200000000000095, 0.35499999999999865, 0.511000000000001, 0.5449999999999999, 0.745000000000001, 0.8230000000000022), 'token_inds': (2, 8, 10, 12, 16, 18, 20, 22, 24, 26, 28), 'durations': [0, 2.368, 0.40200000000000014, 0.6970000000000001, 2.4170000000000003, 2.773, 1.7930000000000001, 2.0389999999999997, 0.35499999999999954, 0.6090000000000009, 0.23199999999999932, 0.45199999999999996, 1.1579999999999995, 1.379999999999999, 1.202, 1.4480000000000004, 0.40000000000000036, 0.5199999999999996, 0.32200000000000095, 0.46300000000000097, 0.35499999999999865, 0.581999999999999, 0.511000000000001, 0.7210000000000001, 0.5449999999999999, 0.8819999999999997, 0.745000000000001, 1.032, 0.8230000000000022, 0], 'step_durations': (2.368, 0.6970000000000001, 2.773, 2.0389999999999997, 0.6090000000000009, 0.45199999999999996, 1.379999999999999, 1.4480000000000004, 0.5199999999999996, 0.46300000000000097, 0.581999999999999, 0.7210000000000001, 0.8819999999999997, 1.032), 'step_inds': (1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27), 'point_inds': [], 'point': [], 'script': {'BOTH': ['오늘1', '시:3시39분', '아파트1', '시장1', '불타다1', '사고1', '길1', '자동차밀리다1', '조심1'], 'STRONG': ['f:관악구', 'f:신림', '장소1', '새벽1', 'f:한신'], 'WEAK': []}}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# .pkl 파일 경로\n",
    "pkl_path = \"data/train_dataset.pkl\"\n",
    "\n",
    "# 파일 열기\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "\n",
    "# obj가 JsonDataset 인스턴스라면 .files 속성에 데이터가 있음\n",
    "if hasattr(obj, 'files'):\n",
    "    print(f\"총 데이터 수: {len(obj.files)}\")\n",
    "    print(\"첫 번째 데이터 샘플:\")\n",
    "    print(obj.files[0])\n",
    "else:\n",
    "    print(\"이 객체는 'files' 속성이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKokxLnMaGDn",
        "outputId": "7fe7d2ae-f273-4079-aeef-b837309367cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dataset\n",
            "  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Collecting sqlalchemy<2.0.0,>=1.3.2 (from dataset)\n",
            "  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting alembic>=0.6.2 (from dataset)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting banal>=1.0.1 (from dataset)\n",
            "  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=0.6.2->dataset) (1.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
            "Downloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
            "Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: banal, sqlalchemy, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, JPype1, nvidia-cusparse-cu12, nvidia-cudnn-cu12, konlpy, alembic, nvidia-cusolver-cu12, dataset, bitsandbytes\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.41\n",
            "    Uninstalling SQLAlchemy-2.0.41:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.41\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed JPype1-1.5.2 alembic-1.16.2 banal-1.0.6 bitsandbytes-0.46.0 dataset-1.6.2 konlpy-0.6.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sqlalchemy-1.4.54\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets sentence-transformers torch accelerate bitsandbytes konlpy dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, MarianMTModel, MarianTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# from tensorflow.keras.datasets import imdb\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import json\n",
        "colab_path = \"/content/drive/MyDrive/Colab Notebooks/KUBIG-25S\"\n",
        "os.chdir(colab_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07VmYnyfwqVb",
        "outputId": "bf6776db-35cb-457d-e696-6ffcea490eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ2dZ0oY9BiR"
      },
      "source": [
        "# 1. Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBwQhvM7Lc9p",
        "outputId": "047d9314-8ae4-46d0-c48d-c050b241cb3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Translating English to Korean ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 77/77 [01:41<00:00,  1.32s/it]\n"
          ]
        }
      ],
      "source": [
        "client_id = \"\"\n",
        "client_secret = \"\"\n",
        "\n",
        "def papago_translate(text, source='en', target='ko'):\n",
        "    url = \"https://papago.apigw.ntruss.com/nmt/v1/translation\"\n",
        "    headers = {\n",
        "        \"X-NCP-APIGW-API-KEY-ID\": client_id,\n",
        "        \"X-NCP-APIGW-API-KEY\": client_secret\n",
        "    }\n",
        "    data = {\n",
        "        \"source\": source,\n",
        "        \"target\": target,\n",
        "        \"text\": text\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, data=data)\n",
        "    return response.json()\n",
        "\n",
        "def prepare_data(start=0, end=5000, max_length = 2000):\n",
        "\n",
        "    (indice_en, labels), (x_test, y_test) = imdb.load_data()\n",
        "    indice_en = np.concatenate([indice_en, x_test])[start:end]\n",
        "    labels_all = np.concatenate([labels, y_test])[start:end]\n",
        "\n",
        "\n",
        "    word_index = imdb.get_word_index()\n",
        "    reverse_index = {value + 3: key for key, value in word_index.items()}\n",
        "    reverse_index[0] = \"<PAD>\"\n",
        "    reverse_index[1] = \"\"\n",
        "    reverse_index[2] = \"<UNK>\"\n",
        "    reverse_index[10] = \"\\n\"\n",
        "\n",
        "    texts_en = []\n",
        "    labels = []\n",
        "    for idx, label in zip(indice_en, labels_all):\n",
        "        text_en = ' '.join([reverse_index.get(i, '?') for i in idx]).strip()\n",
        "        if len(text_en) < max_length:\n",
        "            texts_en.append(text_en)\n",
        "            labels.append(label)\n",
        "\n",
        "    print(\"Translating English to Korean ...\")\n",
        "\n",
        "    texts_ko = []\n",
        "    valid_texts_en = []\n",
        "    valid_labels = []\n",
        "\n",
        "    for text, label in tqdm(zip(texts_en, labels), total=len(texts_en)):\n",
        "        try:\n",
        "            result = papago_translate(text)\n",
        "            translated = result['message']['result']['translatedText']\n",
        "            texts_ko.append(translated)\n",
        "            valid_texts_en.append(text)\n",
        "            valid_labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"[Error] Skipping due to: {e}\")\n",
        "\n",
        "    return pd.DataFrame({\"en\": valid_texts_en, \"ko\": texts_ko, \"label\": valid_labels})\n",
        "\n",
        "test = prepare_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdFnKEG8Qet0"
      },
      "outputs": [],
      "source": [
        "test.to_csv(\"/content/drive/MyDrive/Colab Notebooks/translated_imdb_sample.csv\",index=False)\n",
        "\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/translated_imdb_sample.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HLZCogHNrlw"
      },
      "source": [
        "# 2. Adversarial Learning(번역IMDB-쇼핑리뷰)\n",
        "1. 한국어 번역 데이터(소스)와 한국어 데이터(타겟)으로 embedding과 classifier를 한번에 학습(klue bert)\n",
        "2. classification loss + MMD loss(domain loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRGvWRV7OSHD"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0avWhu2sOSHF"
      },
      "source": [
        "### IMDB(20K; Translated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQNKgjXSWRz6",
        "outputId": "3fdeb88a-d726-4e02-81b9-e5b17fe78b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19525 entries, 0 to 19524\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   en      19525 non-null  object\n",
            " 1   ko      19525 non-null  object\n",
            " 2   label   19525 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 457.7+ KB\n"
          ]
        }
      ],
      "source": [
        "imdb_df = pd.read_csv(\"translated_imdb.csv\").iloc[:20000,:]\n",
        "\n",
        "imdb_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5YGC97eOSHF"
      },
      "source": [
        "### Naver Review(180K + 5K; +Preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgmJLE0yOSHF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "naver = pd.read_csv(\"Train_target.csv\").iloc[:20000,:]\n",
        "naver_val = pd.read_csv(\"Valid_target.csv\").iloc[:5000,:]\n",
        "\n",
        "slang_dict1 = {\n",
        "    \"ㅈㄴ\": \"너무\",\n",
        "    \"ㄹㅇ\": \"정말로\",\n",
        "    \"ㅂㅂ\": \"안녕\",\n",
        "    \"ㅇㅈ\": \"인정\",\n",
        "    \"ㄴㄴ\": \"금지\",\n",
        "    \"ㅋㅋ\": \"키키\",\n",
        "    \"ㅋㅎ\": \"키키\",\n",
        "    \"ㅎㅎ\": \"히히\",\n",
        "    \"ㅎㅋ\": \"히히\",\n",
        "    \"ㅜㅜ\": \"흑흑\",\n",
        "    \"ㅜㅠ\": \"흑흑\",\n",
        "    \"ㅠㅠ\": \"흑흑\",\n",
        "    \"ㅠㅜ\": \"흑흑\",\n",
        "    \"ㅡㅡ\": \"실망\",\n",
        "    \"ㅅㅂ\": \"최악\",\n",
        "    \"ㅆㅂ\": \"최악\",\n",
        "    \"ㅆㅃ\": \"최악\",\n",
        "    \"ㅄ\": \"최악\",\n",
        "    \"ㅂㅅ\": \"최악\",\n",
        "    \"ㅈㄹ\": \"최악\"\n",
        "    # 추가적인 신조어 및 비속어를 여기에 계속 추가\n",
        "}\n",
        "\n",
        "slang_dict2 = {\n",
        "    \"ㅋ\": \"킥\",\n",
        "    \"ㅎ\": \"헤헤\",\n",
        "    \"ㅜ\": \"으앙\",\n",
        "    \"ㅜ\": \"으앙\"\n",
        "    # 추가적인 신조어 및 비속어를 여기에 계속 추가\n",
        "}\n",
        "\n",
        "def preprocessing(text):\n",
        "    for k, v in slang_dict1.items():\n",
        "            text = text.replace(k, v)\n",
        "\n",
        "    for k, v in slang_dict2.items():\n",
        "            text = text.replace(k, v)\n",
        "\n",
        "    return re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text) # 반복표현 제거\n",
        "\n",
        "naver['text'] = naver['text'].apply(preprocessing)\n",
        "naver_val['text'] = naver_val['text'].apply(preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-14NohFO2MS"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsd2bGPjT3ad"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308,
          "referenced_widgets": [
            "13079eb52df544c88ec8ca2244576c52",
            "37dd09622fa74ca7a87149203cb2c080",
            "8ada2935198444f2bdd96b3be732d68c",
            "6e567051a5cc4985befe48bec0cbe4cf",
            "8a9a3b3b2d6f40ca9b2c6d9207676f21",
            "5e8cb565b24b417090e90a40465903aa",
            "c3c26e949ccd4c1a861215921b6cd467",
            "47ab81384ac54a03a098a487bf08b8e1",
            "6833ce2cd27a41c897dc104cba5ca7da",
            "afc7a1075cbe44608138d198f6703ce2",
            "25947bac3f1040059c62aa01af05b27e",
            "83cf66769c95486bb458d26a4decbec8",
            "83f94716fbfd4c9384cf47ab00017ac1",
            "2b28d92efb484a1c9b534435101f5205",
            "0da8295914af4a0eb6ff4cb2553b672e",
            "29d2e76b9c054358ada9749255d54fd6",
            "17ce51c93a1e4aaebc36bc7326bb6a20",
            "6ec688a6967543a8b37ca48b49342b68",
            "ad64771df12b41109b83dde6c498fd5d",
            "ad7936225b744583808b1c5f9b0dff50",
            "d0aebfa300fa428a938c9320ab838985",
            "b92c4597864e4ac698bd6bcd22b6ea80",
            "7ffe124d2f0a4eaa9714e50055c7c913",
            "74752bfd6db94c1798976626eb64af63",
            "0708127a7af14fb683a8faf486b9b4ab",
            "b948765970c84e5c9a7fd1a9c5cc7546",
            "c5be2cbd627b43b388765066ce8e6e4c",
            "fbb57db7c8c0436e88df8a5d662355e7",
            "fa8e7978a89245ef92dbea4f60971f0a",
            "9c87711ad7524c5186edc6a97c7a39b6",
            "474ac25a554243e29fb838e338a97c93",
            "81394917da734fd4aa42df6ebed8b25a",
            "f4a9ff46df68408c804f677cf8443c3c",
            "2e8685f58e49409b8c862e86c6bc6fc3",
            "7225ea6da55542ed9981ac60c2303234",
            "0c9da0ff82814d84b50965a935ce3601",
            "33522ecdf79c4af68d0981257d00ecb3",
            "21ebc2cd9d5d4f45b23f848ddbcb6691",
            "4370da0d889c40509767187b3be9a56f",
            "f53c878fc6eb4e55be64835fb9b67003",
            "816fd5529048443fbc23fa4d9d725358",
            "775a67468a944c52886cf9108fa49af0",
            "73d31e1e88a24f419ed7c71550c7fe5d",
            "fa294e032b6d40d38b4ebc8695c8c8a6",
            "548300775dac4df986db1b37f5e5015f",
            "3ba72062bea145868cd9db2cec3481ae",
            "f69e4ba7edec4ebba8c334bbc67fee48",
            "e05a9a302c874532b4eab5739d993432",
            "0e2a1e49f46a44cf9c110bc26b14dea9",
            "cd14393e917045db8bc216f71540ff9f",
            "551fe00385de42d88c3479d724c3ab22",
            "130f161f46d6468f9690f4bc86fe869f",
            "56e2051eef384a6fa63f93035c9c6f99",
            "a05ea0b3874c4f6b9d9cdc57f6effb4d",
            "6afda7d2ce8d4469984e268cbd3daf8a"
          ]
        },
        "id": "TKucVaWbUgGx",
        "outputId": "7757b8f9-9bc2-45b5-9a14-26216884b0ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13079eb52df544c88ec8ca2244576c52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/425 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83cf66769c95486bb458d26a4decbec8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ffe124d2f0a4eaa9714e50055c7c913"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e8685f58e49409b8c862e86c6bc6fc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "548300775dac4df986db1b37f5e5015f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "class DomainDataset(Dataset):\n",
        "    def __init__(self, texts, domain_labels, tokenizer, max_len=512, sentiment_labels=None):\n",
        "        self.texts = texts\n",
        "        self.domain_labels = domain_labels\n",
        "        self.sentiment_labels = sentiment_labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = self.tokenizer(self.texts[idx], padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        item = {key: val.squeeze(0) for key, val in encoded.items()}\n",
        "        item['domain_label'] = torch.tensor(self.domain_labels[idx])\n",
        "        item['sentiment_label'] = torch.tensor(int(self.sentiment_labels[idx]), dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "src_texts = imdb_df['ko'].tolist()\n",
        "tgt_texts = naver['text'].tolist()\n",
        "src_labels = imdb_df['label'].tolist()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "src_ds = DomainDataset(src_texts, [0] * len(src_texts), tokenizer, sentiment_labels=src_labels)\n",
        "tgt_ds = DomainDataset(tgt_texts, [1] * len(tgt_texts), tokenizer, sentiment_labels=[-1] * len(tgt_texts))\n",
        "val_ds = DomainDataset(naver_val['text'].tolist(), [1] * len(naver_val), tokenizer, sentiment_labels=naver_val['label'].tolist())\n",
        "\n",
        "src_loader = DataLoader(src_ds, batch_size=32, shuffle=True, drop_last=True)\n",
        "tgt_loader = DataLoader(tgt_ds, batch_size=32, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRE2-CQIUkNo"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8Tha0y1e3Xe"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_mmd(x_src, x_tgt, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
        "    batch_size = x_src.size(0)\n",
        "    x = torch.cat([x_src, x_tgt], dim=0)\n",
        "\n",
        "    # Compute pairwise squared distances\n",
        "    xx = torch.matmul(x, x.t())\n",
        "    x2 = torch.sum(x * x, dim=1, keepdim=True)\n",
        "    squared_dist = x2 + x2.t() - 2 * xx\n",
        "\n",
        "    # Compute kernels\n",
        "    if fix_sigma:\n",
        "        bandwidth = fix_sigma\n",
        "    else:\n",
        "        bandwidth = torch.sum(squared_dist.detach()) / (batch_size ** 2 - batch_size)\n",
        "\n",
        "    bandwidth_list = [bandwidth * (kernel_mul ** i) for i in range(kernel_num)]\n",
        "    kernels = [torch.exp(-squared_dist / bw) for bw in bandwidth_list]\n",
        "    kernel_matrix = sum(kernels)\n",
        "\n",
        "    # Split kernel matrix\n",
        "    K_XX = kernel_matrix[:batch_size, :batch_size]\n",
        "    K_YY = kernel_matrix[batch_size:, batch_size:]\n",
        "    K_XY = kernel_matrix[:batch_size, batch_size:]\n",
        "    K_YX = kernel_matrix[batch_size:, :batch_size]\n",
        "\n",
        "    mmd = torch.mean(K_XX + K_YY - K_XY - K_YX)\n",
        "    return mmd\n",
        "\n",
        "class MMDModel(nn.Module):\n",
        "    def __init__(self,hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"klue/bert-base\")\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls = outputs.last_hidden_state[:, 0]  # CLS 토큰\n",
        "        sentiment_logits = self.classifier(cls)\n",
        "        return sentiment_logits, cls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXrzenIZUmzs"
      },
      "source": [
        "> 손실 계수 튜닝 및 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCPjmEPVe4zF"
      },
      "outputs": [],
      "source": [
        "def train_mmd_model(src_loader, tgt_loader, val_loader, num_epochs=5, beta=1.0):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MMDModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    sentiment_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_model_state = None\n",
        "    save_path = f\"models/mmd_model_beta{beta}.pth\"\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_cls_loss, total_mmd_loss = 0.0, 0.0\n",
        "\n",
        "        src_iter = iter(src_loader)\n",
        "        tgt_iter = iter(tgt_loader)\n",
        "        steps_per_epoch = min(len(src_loader), len(tgt_loader))\n",
        "\n",
        "        for step in tqdm(range(steps_per_epoch), desc=f\"Epoch {epoch+1} - Train\"):\n",
        "            try:\n",
        "                src_batch = next(src_iter)\n",
        "                tgt_batch = next(tgt_iter)\n",
        "            except StopIteration:\n",
        "                continue\n",
        "\n",
        "            input_ids_src = src_batch['input_ids'].to(device)\n",
        "            mask_src = src_batch['attention_mask'].to(device)\n",
        "            labels_src = src_batch['sentiment_label'].to(device)\n",
        "\n",
        "            input_ids_tgt = tgt_batch['input_ids'].to(device)\n",
        "            mask_tgt = tgt_batch['attention_mask'].to(device)\n",
        "\n",
        "            logits_src, feat_src = model(input_ids_src, mask_src)\n",
        "            _, feat_tgt = model(input_ids_tgt, mask_tgt)\n",
        "\n",
        "            cls_loss = sentiment_criterion(logits_src, labels_src)\n",
        "            mmd_loss = compute_mmd(feat_src, feat_tgt)\n",
        "\n",
        "            loss = cls_loss + beta * mmd_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_cls_loss += cls_loss.item()\n",
        "            total_mmd_loss += mmd_loss.item()\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] Train Sentiment Loss: {total_cls_loss/steps_per_epoch:.4f} | MMD Loss: {total_mmd_loss/steps_per_epoch:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Valid\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['sentiment_label'].to(device)\n",
        "\n",
        "                logits, _ = model(input_ids, mask)\n",
        "                loss = sentiment_criterion(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = logits.argmax(dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict()\n",
        "            torch.save(best_model_state, save_path)\n",
        "            print(f\">> Saved best model at epoch {epoch+1}\")\n",
        "\n",
        "        # visualization\n",
        "        feats_all, domain_markers, label_colors = [], [], []\n",
        "        model.eval()\n",
        "        src_iter_vis = iter(src_loader)\n",
        "        tgt_iter_vis = iter(val_loader)\n",
        "        steps = 1000 // src_loader.batch_size\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(steps):\n",
        "                try:\n",
        "                    src_batch = next(src_iter_vis)\n",
        "                    tgt_batch = next(tgt_iter_vis)\n",
        "                except StopIteration:\n",
        "                    break\n",
        "\n",
        "                # Source\n",
        "                input_ids_src = src_batch['input_ids'].to(device)\n",
        "                mask_src = src_batch['attention_mask'].to(device)\n",
        "                labels_src = src_batch['sentiment_label'].cpu().numpy()\n",
        "                _, feat_src = model(input_ids_src, mask_src)\n",
        "\n",
        "                feats_all.append(feat_src.cpu().numpy())\n",
        "                domain_markers += ['source'] * len(feat_src)\n",
        "                label_colors += list(labels_src)\n",
        "\n",
        "                # Target\n",
        "                input_ids_tgt = tgt_batch['input_ids'].to(device)\n",
        "                mask_tgt = tgt_batch['attention_mask'].to(device)\n",
        "                labels_tgt = tgt_batch['sentiment_label'].cpu().numpy()\n",
        "                _, feat_tgt = model(input_ids_tgt, mask_tgt)\n",
        "\n",
        "                feats_all.append(feat_tgt.cpu().numpy())\n",
        "                domain_markers += ['target'] * len(feat_tgt)\n",
        "                label_colors += list(labels_tgt)\n",
        "\n",
        "        feats_all = np.vstack(feats_all)\n",
        "        domain_markers = np.array(domain_markers)\n",
        "        label_colors = np.array(label_colors)\n",
        "\n",
        "        tsne = TSNE(n_components=2, perplexity=30, random_state=42, init='pca', learning_rate='auto')\n",
        "        feats_2d = tsne.fit_transform(feats_all)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        classes = np.unique(label_colors)\n",
        "        markers = {'source': 'o', 'target': 'x'}\n",
        "        cmap = plt.get_cmap(\"tab10\", 10*len(classes))\n",
        "\n",
        "        for cls in classes:\n",
        "            for domain in ['source', 'target']:\n",
        "                idxs = (label_colors == cls) & (domain_markers == domain)\n",
        "                plt.scatter(\n",
        "                    feats_2d[idxs, 0], feats_2d[idxs, 1],\n",
        "                    c=[cmap(5*cls)], marker=markers[domain],\n",
        "                    label=f\"{domain}-class{cls}\", alpha=0.6\n",
        "                )\n",
        "\n",
        "        plt.legend()\n",
        "        plt.title(f\"t-SNE Embedding Visualization (beta {beta}, Epoch {epoch+1})\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"visualizations/adversarial_beta{beta}_epoch_{epoch+1}_tsne.png\")\n",
        "        plt.close()\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_initial_embeddings(model, src_loader, tgt_loader, beta=1.0):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    feats_all, domain_markers, label_colors = [], [], []\n",
        "    src_iter = iter(src_loader)\n",
        "    tgt_iter = iter(val_loader)\n",
        "    steps = 1000 // src_loader.batch_size  # 또는 원하는 시각화 샘플 수로 조정\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(steps):\n",
        "            try:\n",
        "                src_batch = next(src_iter)\n",
        "                tgt_batch = next(tgt_iter)\n",
        "            except StopIteration:\n",
        "                break\n",
        "\n",
        "            # Source\n",
        "            input_ids_src = src_batch['input_ids'].to(device)\n",
        "            mask_src = src_batch['attention_mask'].to(device)\n",
        "            labels_src = src_batch['sentiment_label'].cpu().numpy()\n",
        "            _, feat_src = model(input_ids_src, mask_src)\n",
        "\n",
        "            feats_all.append(feat_src.cpu().numpy())\n",
        "            domain_markers += ['source'] * len(feat_src)\n",
        "            label_colors += list(labels_src)\n",
        "\n",
        "            # Target\n",
        "            input_ids_tgt = tgt_batch['input_ids'].to(device)\n",
        "            mask_tgt = tgt_batch['attention_mask'].to(device)\n",
        "            labels_tgt = tgt_batch['sentiment_label'].cpu().numpy()\n",
        "            _, feat_tgt = model(input_ids_tgt, mask_tgt)\n",
        "\n",
        "            feats_all.append(feat_tgt.cpu().numpy())\n",
        "            domain_markers += ['target'] * len(feat_tgt)\n",
        "            label_colors += list(labels_tgt)\n",
        "\n",
        "    # Combine and visualize\n",
        "    feats_all = np.vstack(feats_all)\n",
        "    domain_markers = np.array(domain_markers)\n",
        "    label_colors = np.array(label_colors)\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity=30, random_state=42, init='pca', learning_rate='auto')\n",
        "    feats_2d = tsne.fit_transform(feats_all)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    classes = np.unique(label_colors)\n",
        "    markers = {'source': 'o', 'target': 'x'}\n",
        "    cmap = plt.get_cmap(\"tab10\", 10*len(classes))\n",
        "\n",
        "    for cls in classes:\n",
        "        for domain in ['source', 'target']:\n",
        "            idxs = (label_colors == cls) & (domain_markers == domain)\n",
        "            plt.scatter(\n",
        "                feats_2d[idxs, 0], feats_2d[idxs, 1],\n",
        "                c=[cmap(5*cls)], marker=markers[domain],\n",
        "                label=f\"{domain}-class{cls}\", alpha=0.6\n",
        "            )\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(f\"t-SNE Embedding Visualization (Initial, beta {beta})\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"visualizations/adversarial_beta{beta}_epoch_0_tsne.png\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "111fmOpANzd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MMDModel()\n",
        "visualize_initial_embeddings(model, src_loader, tgt_loader, beta=1.0)"
      ],
      "metadata": {
        "id": "IoKft7FJN0zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuIiefKXhWkd",
        "outputId": "58d79b87-c4ad-4564-fb98-97a0cdd37005"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train: 100%|██████████| 610/610 [12:06<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Sentiment Loss: 0.3045 | MMD Loss: 0.0329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Val Loss: 0.2574, Accuracy: 0.8998\n",
            ">> Saved best model at epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Train: 100%|██████████| 610/610 [12:06<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train Sentiment Loss: 0.1758 | MMD Loss: 0.0273\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Val Loss: 0.2655, Accuracy: 0.8974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Train: 100%|██████████| 610/610 [12:05<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train Sentiment Loss: 0.0975 | MMD Loss: 0.0297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Val Loss: 0.2856, Accuracy: 0.9002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Train: 100%|██████████| 610/610 [12:06<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train Sentiment Loss: 0.0607 | MMD Loss: 0.0293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Val Loss: 0.4319, Accuracy: 0.8844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Train: 100%|██████████| 610/610 [12:05<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train Sentiment Loss: 0.0437 | MMD Loss: 0.0284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Val Loss: 0.3589, Accuracy: 0.8980\n"
          ]
        }
      ],
      "source": [
        "train_mmd_model(src_loader, tgt_loader, val_loader, beta=1.0)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpEmlL79niMy",
        "outputId": "e79e734f-0310-4ec6-d877-81ee0daea715"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train: 100%|██████████| 610/610 [12:06<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Sentiment Loss: 0.3123 | MMD Loss: 0.0321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Val Loss: 0.2639, Accuracy: 0.9030\n",
            ">> Saved best model at epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Train: 100%|██████████| 610/610 [12:06<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train Sentiment Loss: 0.1698 | MMD Loss: 0.0297\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Val Loss: 0.2753, Accuracy: 0.8996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Train: 100%|██████████| 610/610 [12:06<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train Sentiment Loss: 0.0950 | MMD Loss: 0.0289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Val Loss: 0.3469, Accuracy: 0.8962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Train: 100%|██████████| 610/610 [12:06<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train Sentiment Loss: 0.0538 | MMD Loss: 0.0284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Val Loss: 0.3830, Accuracy: 0.8964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Train: 100%|██████████| 610/610 [12:06<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train Sentiment Loss: 0.0375 | MMD Loss: 0.0280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Val Loss: 0.3674, Accuracy: 0.8936\n"
          ]
        }
      ],
      "source": [
        "train_mmd_model(src_loader, tgt_loader, val_loader, beta=0.8)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coyXH2LanjZq",
        "outputId": "7455af85-31ad-4367-e5d2-c2c28601e796"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Sentiment Loss: 0.2995 | MMD Loss: 0.0341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Val Loss: 0.2651, Accuracy: 0.9006\n",
            ">> Saved best model at epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train Sentiment Loss: 0.1695 | MMD Loss: 0.0285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Val Loss: 0.2557, Accuracy: 0.9058\n",
            ">> Saved best model at epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train Sentiment Loss: 0.0886 | MMD Loss: 0.0290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Val Loss: 0.3167, Accuracy: 0.9018\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train Sentiment Loss: 0.0538 | MMD Loss: 0.0290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Val Loss: 0.3579, Accuracy: 0.9042\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train Sentiment Loss: 0.0436 | MMD Loss: 0.0275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Val Loss: 0.3886, Accuracy: 0.9034\n"
          ]
        }
      ],
      "source": [
        "train_mmd_model(src_loader, tgt_loader, val_loader, beta=0.6)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-tvhknRnk8I",
        "outputId": "0558bcdf-b98b-4478-adad-1694648ab2f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Sentiment Loss: 0.3245 | MMD Loss: 0.0340\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Val Loss: 0.2524, Accuracy: 0.9078\n",
            ">> Saved best model at epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train Sentiment Loss: 0.1832 | MMD Loss: 0.0282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Val Loss: 0.2535, Accuracy: 0.9058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train Sentiment Loss: 0.1040 | MMD Loss: 0.0275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 3] Val Loss: 0.2917, Accuracy: 0.8954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train Sentiment Loss: 0.0591 | MMD Loss: 0.0292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 4] Val Loss: 0.2975, Accuracy: 0.9044\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Train: 100%|██████████| 610/610 [12:07<00:00,  1.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train Sentiment Loss: 0.0416 | MMD Loss: 0.0288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Valid: 100%|██████████| 157/157 [00:31<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 5] Val Loss: 0.3798, Accuracy: 0.8992\n"
          ]
        }
      ],
      "source": [
        "train_mmd_model(src_loader, tgt_loader, val_loader, beta=1.2)\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}